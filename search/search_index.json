{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Materials Project Workshop 2020 (July 28 - 30) \u00b6 The 2020 Materials Project Workshop will be fully virtual due to COVID related travel and congregation restrictions. We'll use a cloud-based Jupyter deployment to enable interactive tutorials where we provide close support. Registration \u00b6 Registration is currently waitlisted. Tentative Schedule \u00b6 Primer \u00b6 (optional) Tuesday Morning -- Python/Jupyter Primer \u00b6 09:00-09:20 Intro - Logistics 09:20-10:20 Basics - Variables, Lists, Loops 10:20-10:40 Break 10:40-11:40 Control: Conditions, Funtions, Sets/Dictionaries 11:40-12:00 Open Discussion 12:00-13:00 Lunch (optional) Tuesday Afternoon -- MongoDB using Python primer \u00b6 13:00-13:20 Data Modeling - Python Types to JSON to MongoDB 13:20-14:00 Listing, Counting, Finding, and Dot Notation 14:00-14:20 Break 14:20-15:20 Advanced Filtering, Basic Aggregation, and Indexing 15:20-15:40 Open Discussion 15:40-16:00 Wrap-Up Main Workshop \u00b6 Wednesday Morning \u00b6 09:00-09:20 Overview of the Materials Project 09:20-10:20 Using the Website Effectively 10:20-10:40 Break 10:40-11:40 Pymatgen Foundations 11:40-12:00 Open Discussion 12:00-13:00 Lunch Wednesday Afternoon \u00b6 13:00-14:00 Working with Surfaces in Pymatgen 14:00-14:20 Break 14:20-15:20 Using the Materials API 15:20-15:40 Open Discussion 15:40-16:00 Wrap-up Thursday Morning \u00b6 09:00-09:20 Intro 09:20-10:20 Running Other Codes 10:20-10:40 Break 10:40-11:40 Exploring New Systems with Pymatgen 11:40-12:00 Open Discussion 12:00-13:00 Lunch Thursday Afternoon \u00b6 13:00-14:00 MPContribs : Share You Data via MPContribs 14:00-14:20 Break 14:20-15:40 Machine Learning with Matminer 15:40-16:00 Wrap-Up Instructions: \u00b6 Sign in to your MP account online. Register for an account first if needed. More instructions are coming as the workshop tools are finalized. Format \u00b6 The lessons will be taught in 20-minute sessions as 10-minutes of \"lecture\" followed by 10-minutes of a hands-on exercise. You're welcome to follow along with the already filled in hand-out notebook or by typing in as we go along. Still have questions? \u00b6 If you have any further questions, you can find some more helpful info and confact information here","title":"Home"},{"location":"#materials-project-workshop-2020-july-28-30","text":"The 2020 Materials Project Workshop will be fully virtual due to COVID related travel and congregation restrictions. We'll use a cloud-based Jupyter deployment to enable interactive tutorials where we provide close support.","title":"Materials Project Workshop 2020 (July 28 - 30)"},{"location":"#registration","text":"Registration is currently waitlisted.","title":"Registration"},{"location":"#tentative-schedule","text":"","title":"Tentative Schedule"},{"location":"#primer","text":"","title":"Primer"},{"location":"#optional-tuesday-morning-pythonjupyter-primer","text":"09:00-09:20 Intro - Logistics 09:20-10:20 Basics - Variables, Lists, Loops 10:20-10:40 Break 10:40-11:40 Control: Conditions, Funtions, Sets/Dictionaries 11:40-12:00 Open Discussion 12:00-13:00 Lunch","title":"(optional) Tuesday Morning -- Python/Jupyter Primer"},{"location":"#optional-tuesday-afternoon-mongodb-using-python-primer","text":"13:00-13:20 Data Modeling - Python Types to JSON to MongoDB 13:20-14:00 Listing, Counting, Finding, and Dot Notation 14:00-14:20 Break 14:20-15:20 Advanced Filtering, Basic Aggregation, and Indexing 15:20-15:40 Open Discussion 15:40-16:00 Wrap-Up","title":"(optional) Tuesday Afternoon -- MongoDB using Python primer"},{"location":"#main-workshop","text":"","title":"Main Workshop"},{"location":"#wednesday-morning","text":"09:00-09:20 Overview of the Materials Project 09:20-10:20 Using the Website Effectively 10:20-10:40 Break 10:40-11:40 Pymatgen Foundations 11:40-12:00 Open Discussion 12:00-13:00 Lunch","title":"Wednesday Morning"},{"location":"#wednesday-afternoon","text":"13:00-14:00 Working with Surfaces in Pymatgen 14:00-14:20 Break 14:20-15:20 Using the Materials API 15:20-15:40 Open Discussion 15:40-16:00 Wrap-up","title":"Wednesday Afternoon"},{"location":"#thursday-morning","text":"09:00-09:20 Intro 09:20-10:20 Running Other Codes 10:20-10:40 Break 10:40-11:40 Exploring New Systems with Pymatgen 11:40-12:00 Open Discussion 12:00-13:00 Lunch","title":"Thursday Morning"},{"location":"#thursday-afternoon","text":"13:00-14:00 MPContribs : Share You Data via MPContribs 14:00-14:20 Break 14:20-15:40 Machine Learning with Matminer 15:40-16:00 Wrap-Up","title":"Thursday Afternoon"},{"location":"#instructions","text":"Sign in to your MP account online. Register for an account first if needed. More instructions are coming as the workshop tools are finalized.","title":"Instructions:"},{"location":"#format","text":"The lessons will be taught in 20-minute sessions as 10-minutes of \"lecture\" followed by 10-minutes of a hands-on exercise. You're welcome to follow along with the already filled in hand-out notebook or by typing in as we go along.","title":"Format"},{"location":"#still-have-questions","text":"If you have any further questions, you can find some more helpful info and confact information here","title":"Still have questions?"},{"location":"08_ml_matminer/unit-1-exercises-answers/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Materials data science: data retrieval and filtering \u2013 Exercises \u00b6 Exercise 1: Load and examine the elastic_tensor_2015 dataset \u00b6 Matminer includes a dataset called elastic_tensor_2015 . It contains a set of computed elastic properties of materials sourced from the paper: \"Charting the complete elastic properties of inorganic crystalline compounds\", M. de Jong et al., Sci. Data. 2 (2015) 150009.\" Load this dataset using the load_dataset() function and determine: - the number of entries it contains (tip: pandas DataFrame objects have a describe() function) - the largest value of bulk modulus in the dataset (bulk modulus is given in the K_VRH column) from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) df . describe () Exercise 2: Filter the dataset based on the number of sites \u00b6 You are constructing a machine learning model for elastic constants that is only designed to be employed on structures containing a small number of atomic sites. You should filter the dataset to only include entries where nsites is less than 20 and determine: - the number of entries in the filtered dataset - the mean number of sites across all entries in your filtered dataset from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below mask = df [ \"nsites\" ] < 20 filtered_df = df [ mask ] n_filtered = len ( filtered_df ) mean_n_sites = filtered_df [ \"nsites\" ] . mean () print ( \"# entries in filtered dataset: {} \" . format ( n_filtered )) print ( \"mean # sites: {} \" . format ( mean_n_sites )) Exercise 3: Remove columns unnecessary for machine learning \u00b6 The elastic tensor dataset contains many columns that are not particular relevant for machine learning. You should filter the dataset, so that it only contains the formula , structure , and K_VRH (bulk modulus) columns. Tip: the pandas DataFrame objects implement a drop() function that can be used for dropping both rows and columns. Make sure you set the axis argument correctly. from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below df . drop ([ 'material_id' , 'nsites' , 'space_group' , 'volume' , 'elastic_anisotropy' , 'G_Reuss' , 'G_VRH' , 'G_Voigt' , 'K_Reuss' , 'K_Voigt' , 'poisson_ratio' , 'compliance_tensor' , 'elastic_tensor' , 'elastic_tensor_original' , 'cif' , 'kpoint_density' , 'poscar' ], axis = 1 ) Advanced exercise: calculate Young's modulus \u00b6 Young's modulus, $E$, is given by: $$ E = \\frac{9KG}{G+3K}, $$ where $K$ is the bulk modulus (column K_VRH ), and $G$ is the shear modulus (column G_VRH ). Calculate Young's modulus for all entries in the dataset and store them in a new column called E_VRH . What is the average Young modulus over the entire dataset? from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below df [ \"E_VRH\" ] = ( 9 * df [ \"K_VRH\" ] * df [ \"G_VRH\" ]) / ( df [ \"G_VRH\" ] + 3 * df [ \"K_VRH\" ]) df","title":"Unit 1 exercises answers"},{"location":"08_ml_matminer/unit-1-exercises-answers/#materials-data-science-data-retrieval-and-filtering-exercises","text":"","title":"Materials data science: data retrieval and filtering \u2013\u00a0Exercises"},{"location":"08_ml_matminer/unit-1-exercises-answers/#exercise-1-load-and-examine-the-elastic_tensor_2015-dataset","text":"Matminer includes a dataset called elastic_tensor_2015 . It contains a set of computed elastic properties of materials sourced from the paper: \"Charting the complete elastic properties of inorganic crystalline compounds\", M. de Jong et al., Sci. Data. 2 (2015) 150009.\" Load this dataset using the load_dataset() function and determine: - the number of entries it contains (tip: pandas DataFrame objects have a describe() function) - the largest value of bulk modulus in the dataset (bulk modulus is given in the K_VRH column) from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) df . describe ()","title":"Exercise 1: Load and examine the elastic_tensor_2015 dataset"},{"location":"08_ml_matminer/unit-1-exercises-answers/#exercise-2-filter-the-dataset-based-on-the-number-of-sites","text":"You are constructing a machine learning model for elastic constants that is only designed to be employed on structures containing a small number of atomic sites. You should filter the dataset to only include entries where nsites is less than 20 and determine: - the number of entries in the filtered dataset - the mean number of sites across all entries in your filtered dataset from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below mask = df [ \"nsites\" ] < 20 filtered_df = df [ mask ] n_filtered = len ( filtered_df ) mean_n_sites = filtered_df [ \"nsites\" ] . mean () print ( \"# entries in filtered dataset: {} \" . format ( n_filtered )) print ( \"mean # sites: {} \" . format ( mean_n_sites ))","title":"Exercise 2: Filter the dataset based on the number of sites"},{"location":"08_ml_matminer/unit-1-exercises-answers/#exercise-3-remove-columns-unnecessary-for-machine-learning","text":"The elastic tensor dataset contains many columns that are not particular relevant for machine learning. You should filter the dataset, so that it only contains the formula , structure , and K_VRH (bulk modulus) columns. Tip: the pandas DataFrame objects implement a drop() function that can be used for dropping both rows and columns. Make sure you set the axis argument correctly. from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below df . drop ([ 'material_id' , 'nsites' , 'space_group' , 'volume' , 'elastic_anisotropy' , 'G_Reuss' , 'G_VRH' , 'G_Voigt' , 'K_Reuss' , 'K_Voigt' , 'poisson_ratio' , 'compliance_tensor' , 'elastic_tensor' , 'elastic_tensor_original' , 'cif' , 'kpoint_density' , 'poscar' ], axis = 1 )","title":"Exercise 3: Remove columns unnecessary for machine learning"},{"location":"08_ml_matminer/unit-1-exercises-answers/#advanced-exercise-calculate-youngs-modulus","text":"Young's modulus, $E$, is given by: $$ E = \\frac{9KG}{G+3K}, $$ where $K$ is the bulk modulus (column K_VRH ), and $G$ is the shear modulus (column G_VRH ). Calculate Young's modulus for all entries in the dataset and store them in a new column called E_VRH . What is the average Young modulus over the entire dataset? from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below df [ \"E_VRH\" ] = ( 9 * df [ \"K_VRH\" ] * df [ \"G_VRH\" ]) / ( df [ \"G_VRH\" ] + 3 * df [ \"K_VRH\" ]) df","title":"Advanced exercise: calculate Young's modulus"},{"location":"08_ml_matminer/unit-1-exercises/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Materials data science: data retrieval and filtering \u2013 Exercises \u00b6 Exercise 1: Load and examine the elastic_tensor_2015 dataset \u00b6 Matminer includes a dataset called elastic_tensor_2015 . It contains a set of computed elastic properties of materials sourced from the paper: \"Charting the complete elastic properties of inorganic crystalline compounds\", M. de Jong et al., Sci. Data. 2 (2015) 150009.\" Load this dataset using the load_dataset() function and determine: - the number of entries it contains (tip: pandas DataFrame objects have a describe() function) - the largest value of bulk modulus in the dataset (bulk modulus is given in the K_VRH column) from matminer.datasets import load_dataset df = load_dataset ( ___ ) Exercise 2: Filter the dataset based on the number of sites \u00b6 You are constructing a machine learning model for elastic constants that is only designed to be employed on structures containing a small number of atomic sites. You should filter the dataset to only include entries where nsites is less than 20 and determine: - the number of entries in the filtered dataset - the average number of sites across all entries in your filtered dataset from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below Exercise 3: Remove columns unnecessary for machine learning \u00b6 The elastic tensor dataset contains many columns that are not particular relevant for machine learning. You should filter the dataset, so that it only contains the formula , structure , and K_VRH (bulk modulus) columns. Tip: the pandas DataFrame objects implement a drop() function that can be used for dropping both rows and columns. Make sure you set the axis argument correctly. from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below Advanced exercise: calculate Young's modulus \u00b6 Young's modulus, $E$, is given by: $$ E = \\frac{9KG}{G+3K}, $$ where $K$ is the bulk modulus (column K_VRH ), and $G$ is the shear modulus (column G_VRH ). Calculate Young's modulus for all entries in the dataset and store them in a new column called E_VRH . What is the average Young modulus over the entire dataset? from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below","title":"Unit 1 exercises"},{"location":"08_ml_matminer/unit-1-exercises/#materials-data-science-data-retrieval-and-filtering-exercises","text":"","title":"Materials data science: data retrieval and filtering \u2013\u00a0Exercises"},{"location":"08_ml_matminer/unit-1-exercises/#exercise-1-load-and-examine-the-elastic_tensor_2015-dataset","text":"Matminer includes a dataset called elastic_tensor_2015 . It contains a set of computed elastic properties of materials sourced from the paper: \"Charting the complete elastic properties of inorganic crystalline compounds\", M. de Jong et al., Sci. Data. 2 (2015) 150009.\" Load this dataset using the load_dataset() function and determine: - the number of entries it contains (tip: pandas DataFrame objects have a describe() function) - the largest value of bulk modulus in the dataset (bulk modulus is given in the K_VRH column) from matminer.datasets import load_dataset df = load_dataset ( ___ )","title":"Exercise 1: Load and examine the elastic_tensor_2015 dataset"},{"location":"08_ml_matminer/unit-1-exercises/#exercise-2-filter-the-dataset-based-on-the-number-of-sites","text":"You are constructing a machine learning model for elastic constants that is only designed to be employed on structures containing a small number of atomic sites. You should filter the dataset to only include entries where nsites is less than 20 and determine: - the number of entries in the filtered dataset - the average number of sites across all entries in your filtered dataset from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below","title":"Exercise 2: Filter the dataset based on the number of sites"},{"location":"08_ml_matminer/unit-1-exercises/#exercise-3-remove-columns-unnecessary-for-machine-learning","text":"The elastic tensor dataset contains many columns that are not particular relevant for machine learning. You should filter the dataset, so that it only contains the formula , structure , and K_VRH (bulk modulus) columns. Tip: the pandas DataFrame objects implement a drop() function that can be used for dropping both rows and columns. Make sure you set the axis argument correctly. from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below","title":"Exercise 3: Remove columns unnecessary for machine learning"},{"location":"08_ml_matminer/unit-1-exercises/#advanced-exercise-calculate-youngs-modulus","text":"Young's modulus, $E$, is given by: $$ E = \\frac{9KG}{G+3K}, $$ where $K$ is the bulk modulus (column K_VRH ), and $G$ is the shear modulus (column G_VRH ). Calculate Young's modulus for all entries in the dataset and store them in a new column called E_VRH . What is the average Young modulus over the entire dataset? from matminer.datasets import load_dataset df = load_dataset ( \"elastic_tensor_2015\" ) # complete exercise below","title":"Advanced exercise: calculate Young's modulus"},{"location":"08_ml_matminer/unit-1-notes-blank/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Materials data science: descriptors and machine learning \u00b6 Welcome to the materials data science lesson. In this session, we will demonstrate how to use matminer , pandas and scikit-learn for machine learning materials properties. The lesson is split into three sections: 1. Data retrieval and basic analysis of pandas DataFrame objects. 2. Generating machine learnable descriptors. 3. Training, testing and visualizing machine learning methods with scikit-learn and FigRecipes . Many more tutorials on how to use matminer (beyond the scope of this workshop) are available in the matminer_examples repository, available here . Machine learning workflow \u00b6 Firstly, what does a typical machine learning workflow look like? The overall process can be summarized as: 1. Take raw inputs, such as a list of compositions, and an associated target property to learn. 2. Convert the raw inputs into descriptors or features that can be learned by machine learning algorithms. 3. Train a machine learning model on the data. 4. Plot and analyze the performance of the model. Typically, questions asked by a new practitioner in the field include: - Where do we get the raw data from? - How do we convert the raw data into learnable features? - How can we plot and interpret the results of a model? The `matminer` package has been developed to help make machine learning of materials properties easy and hassle free. The aim of matminer is to connect materials data with data mining algorithms and data visualization. Part 1: Data retrieval and filtering \u00b6 Matminer interfaces with many materials databases, including: - Materials Project - Citrine - AFLOW - Materials Data Facility (MDF) - Materials Platform for Data Science (MPDS) In addition, it also includes datasets from published literature. Matminer hosts a repository of 26 (and growing) datasets which comes from published and peer-reviewed machine learning investigations of materials properties or publications of high-throughput computing studies. In this section, we will show how to access and manipulate the datasets from the published literature. More information on accessing other materials databases are detailed in the matminer_examples repository. A list of the literature-based datasets can be printed using the get_available_datasets() function. This also prints information about what the dataset contains, such as the number of samples, the target properties, and how the data was obtained (e.g., via theory or experiment). from matminer.datasets import get_available_datasets Datasets can be loaded using the load_dataset() function and the database name. To save installation space, the datasets are not automatically downloaded when matminer is installed. Instead, the first time the dataset is loaded, it will be downloaded from the internet and stored in the matminer installation directory. Let's load the dielectric_constant dataset. It contains 1,056 structures with dielectric properties calculated with DFPT-PBE. from matminer.datasets import load_dataset Manipulating and examining pandas DataFrame objects \u00b6 The datasets are made available as pandas DataFrame objects. You can think of these as a type of \"spreadsheet\" object in Python. DataFrames have several useful methods you can use to explore and clean the data, some of which we'll explore below. Inspecting the dataset \u00b6 The head() function prints a summary of the first few rows of a data set. You can scroll across to see more columns. From this, it is easy to see the types of data available in in the dataset. Sometimes, if a dataset is very large, you will be unable to see all the available columns. Instead, you can see the full list of columns using the columns attribute: A pandas DataFrame includes a function called describe() that helps determine statistics for the various numerical/categorical columns in the data. Note that the describe() function only describes numerical columns by default. Sometimes, the describe() function will reveal outliers that indicate mistakes in the data. Indexing the dataset \u00b6 We can access a particular column of DataFrame by indexing the object using the column name. For example: Alternatively, we can access a particular row of a Dataframe using the iloc attribute. Filtering the dataset \u00b6 Pandas DataFrame objects make it very easy to filter the data based on a specific column. We can use the typical Python comparison operators (==, >, >=, <, etc) to filter numerical values. For example, let's find all entries where the cell volume is greater than 580. We do this by filtering on the volume column. Note that we first produce a boolean mask \u2013 a series of True and False depending on the comparison. We can then use the mask to filter the DataFrame . We can use this method of filtering to clean our dataset. For example, if we only wanted our dataset to include semiconductors (materials with a non-zero band gap), we can do this easily by filtering the band_gap column. Often, a dataset contains many additional columns that are not necessary for machine learning. Before we can train a model on the data, we need to remove any extraneous columns. We can remove whole columns from the dataset using the drop() function. This function can be used to drop both rows and columns. The function takes a list of items to drop. For columns, this is column names whereas for rows it is the row number. Finally, the axis option specifies whether the data to drop is columns ( 1 ) or rows ( 0 ). For example, to remove the nsites , space_group , e_electronic , and e_total columns, we can run: Let's examine the cleaned DataFrame to see that the columns have been removed. Generating new columns \u00b6 Pandas DataFrame objects also make it easy to perform simple calculations on the data. Think of this as using formulas in Excel spreadsheets. All fundamental Python math operators (such as +, -, /, and *) can be used. For example, the dielectric dataset contains the electronic contribution to the dielectric constant ($\\epsilon_\\mathrm{electronic}$, in the poly_electronic column) and the total (static) dielectric constant ($\\epsilon_\\mathrm{total}$, in the poly_total column). The ionic contribution to the dataset is given by: $$ \\epsilon_\\mathrm{ionic} = \\epsilon_\\mathrm{total} - \\epsilon_\\mathrm{electronic} $$ Below, we calculate the ionic contribution to the dielectric constant and store it in a new column called poly_ionic . This is as simple as assigning the data to the new column, even if the column doesn't already exist. Let's check the new data was added correctly. Let's practice! \u00b6 Now, let's practice. You'll download a dataset, inspect it, and make sure it is ready to be used for machine learning.","title":"Unit 1 notes blank"},{"location":"08_ml_matminer/unit-1-notes-blank/#materials-data-science-descriptors-and-machine-learning","text":"Welcome to the materials data science lesson. In this session, we will demonstrate how to use matminer , pandas and scikit-learn for machine learning materials properties. The lesson is split into three sections: 1. Data retrieval and basic analysis of pandas DataFrame objects. 2. Generating machine learnable descriptors. 3. Training, testing and visualizing machine learning methods with scikit-learn and FigRecipes . Many more tutorials on how to use matminer (beyond the scope of this workshop) are available in the matminer_examples repository, available here .","title":"Materials data science: descriptors and machine learning"},{"location":"08_ml_matminer/unit-1-notes-blank/#machine-learning-workflow","text":"Firstly, what does a typical machine learning workflow look like? The overall process can be summarized as: 1. Take raw inputs, such as a list of compositions, and an associated target property to learn. 2. Convert the raw inputs into descriptors or features that can be learned by machine learning algorithms. 3. Train a machine learning model on the data. 4. Plot and analyze the performance of the model. Typically, questions asked by a new practitioner in the field include: - Where do we get the raw data from? - How do we convert the raw data into learnable features? - How can we plot and interpret the results of a model? The `matminer` package has been developed to help make machine learning of materials properties easy and hassle free. The aim of matminer is to connect materials data with data mining algorithms and data visualization.","title":"Machine learning workflow"},{"location":"08_ml_matminer/unit-1-notes-blank/#part-1-data-retrieval-and-filtering","text":"Matminer interfaces with many materials databases, including: - Materials Project - Citrine - AFLOW - Materials Data Facility (MDF) - Materials Platform for Data Science (MPDS) In addition, it also includes datasets from published literature. Matminer hosts a repository of 26 (and growing) datasets which comes from published and peer-reviewed machine learning investigations of materials properties or publications of high-throughput computing studies. In this section, we will show how to access and manipulate the datasets from the published literature. More information on accessing other materials databases are detailed in the matminer_examples repository. A list of the literature-based datasets can be printed using the get_available_datasets() function. This also prints information about what the dataset contains, such as the number of samples, the target properties, and how the data was obtained (e.g., via theory or experiment). from matminer.datasets import get_available_datasets Datasets can be loaded using the load_dataset() function and the database name. To save installation space, the datasets are not automatically downloaded when matminer is installed. Instead, the first time the dataset is loaded, it will be downloaded from the internet and stored in the matminer installation directory. Let's load the dielectric_constant dataset. It contains 1,056 structures with dielectric properties calculated with DFPT-PBE. from matminer.datasets import load_dataset","title":"Part 1: Data retrieval and filtering"},{"location":"08_ml_matminer/unit-1-notes-blank/#manipulating-and-examining-pandas-dataframe-objects","text":"The datasets are made available as pandas DataFrame objects. You can think of these as a type of \"spreadsheet\" object in Python. DataFrames have several useful methods you can use to explore and clean the data, some of which we'll explore below.","title":"Manipulating and examining pandas DataFrame objects"},{"location":"08_ml_matminer/unit-1-notes-blank/#inspecting-the-dataset","text":"The head() function prints a summary of the first few rows of a data set. You can scroll across to see more columns. From this, it is easy to see the types of data available in in the dataset. Sometimes, if a dataset is very large, you will be unable to see all the available columns. Instead, you can see the full list of columns using the columns attribute: A pandas DataFrame includes a function called describe() that helps determine statistics for the various numerical/categorical columns in the data. Note that the describe() function only describes numerical columns by default. Sometimes, the describe() function will reveal outliers that indicate mistakes in the data.","title":"Inspecting the dataset"},{"location":"08_ml_matminer/unit-1-notes-blank/#indexing-the-dataset","text":"We can access a particular column of DataFrame by indexing the object using the column name. For example: Alternatively, we can access a particular row of a Dataframe using the iloc attribute.","title":"Indexing the dataset"},{"location":"08_ml_matminer/unit-1-notes-blank/#filtering-the-dataset","text":"Pandas DataFrame objects make it very easy to filter the data based on a specific column. We can use the typical Python comparison operators (==, >, >=, <, etc) to filter numerical values. For example, let's find all entries where the cell volume is greater than 580. We do this by filtering on the volume column. Note that we first produce a boolean mask \u2013 a series of True and False depending on the comparison. We can then use the mask to filter the DataFrame . We can use this method of filtering to clean our dataset. For example, if we only wanted our dataset to include semiconductors (materials with a non-zero band gap), we can do this easily by filtering the band_gap column. Often, a dataset contains many additional columns that are not necessary for machine learning. Before we can train a model on the data, we need to remove any extraneous columns. We can remove whole columns from the dataset using the drop() function. This function can be used to drop both rows and columns. The function takes a list of items to drop. For columns, this is column names whereas for rows it is the row number. Finally, the axis option specifies whether the data to drop is columns ( 1 ) or rows ( 0 ). For example, to remove the nsites , space_group , e_electronic , and e_total columns, we can run: Let's examine the cleaned DataFrame to see that the columns have been removed.","title":"Filtering the dataset"},{"location":"08_ml_matminer/unit-1-notes-blank/#generating-new-columns","text":"Pandas DataFrame objects also make it easy to perform simple calculations on the data. Think of this as using formulas in Excel spreadsheets. All fundamental Python math operators (such as +, -, /, and *) can be used. For example, the dielectric dataset contains the electronic contribution to the dielectric constant ($\\epsilon_\\mathrm{electronic}$, in the poly_electronic column) and the total (static) dielectric constant ($\\epsilon_\\mathrm{total}$, in the poly_total column). The ionic contribution to the dataset is given by: $$ \\epsilon_\\mathrm{ionic} = \\epsilon_\\mathrm{total} - \\epsilon_\\mathrm{electronic} $$ Below, we calculate the ionic contribution to the dielectric constant and store it in a new column called poly_ionic . This is as simple as assigning the data to the new column, even if the column doesn't already exist. Let's check the new data was added correctly.","title":"Generating new columns"},{"location":"08_ml_matminer/unit-1-notes-blank/#lets-practice","text":"Now, let's practice. You'll download a dataset, inspect it, and make sure it is ready to be used for machine learning.","title":"Let's practice!"},{"location":"08_ml_matminer/unit-1-notes/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Materials data science: descriptors and machine learning \u00b6 Welcome to the materials data science lesson. In this session, we will demonstrate how to use matminer , pandas and scikit-learn for machine learning materials properties. The lesson is split into three sections: 1. Data retrieval and basic analysis of pandas DataFrame objects. 2. Generating machine learnable descriptors. 3. Training, testing and visualizing machine learning methods with scikit-learn and FigRecipes . 4. AMM? Many more tutorials on how to use matminer (beyond the scope of this workshop) are available in the matminer_examples repository, available here . Machine learning workflow \u00b6 Firstly, what does a typical machine learning workflow look like? The overall process can be summarized as: 1. Take raw inputs, such as a list of compositions, and an associated target property to learn. 2. Convert the raw inputs into descriptors or features that can be learned by machine learning algorithms. 3. Train a machine learning model on the data. 4. Plot and analyze the performance of the model. Typically, questions asked by a new practitioner in the field include: - Where do we get the raw data from? - How do we convert the raw data into learnable features? - How can we plot and interpret the results of a model? The `matminer` package has been developed to help make machine learning of materials properties easy and hassle free. The aim of matminer is to connect materials data with data mining algorithms and data visualization. Part 1: Data retrieval and filtering \u00b6 Matminer interfaces with many materials databases, including: - Materials Project - Citrine - AFLOW - Materials Data Facility (MDF) - Materials Platform for Data Science (MPDS) In addition, it also includes datasets from published literature. Matminer hosts a repository of 26 (and growing) datasets which comes from published and peer-reviewed machine learning investigations of materials properties or publications of high-throughput computing studies. In this section, we will show how to access and manipulate the datasets from the published literature. More information on accessing other materials databases are detailed in the matminer_examples repository. A list of the literature-based datasets can be printed using the get_available_datasets() function. This also prints information about what the dataset contains, such as the number of samples, the target properties, and how the data was obtained (e.g., via theory or experiment). from matminer.datasets import get_available_datasets get_available_datasets () Datasets can be loaded using the load_dataset() function and the database name. To save installation space, the datasets are not automatically downloaded when matminer is installed. Instead, the first time the dataset is loaded, it will be downloaded from the internet and stored in the matminer installation directory. Let's load the dielectric_constant dataset. It contains 1,056 structures with dielectric properties calculated with DFPT-PBE. from matminer.datasets import load_dataset df = load_dataset ( \"dielectric_constant\" ) Manipulating and examining pandas DataFrame objects \u00b6 The datasets are made available as pandas DataFrame objects. You can think of these as a type of \"spreadsheet\" object in Python. DataFrames have several useful methods you can use to explore and clean the data, some of which we'll explore below. Inspecting the dataset \u00b6 The head() function prints a summary of the first few rows of a data set. You can scroll across to see more columns. From this, it is easy to see the types of data available in in the dataset. df . head () Sometimes, if a dataset is very large, you will be unable to see all the available columns. Instead, you can see the full list of columns using the columns attribute: df . columns A pandas DataFrame includes a function called describe() that helps determine statistics for the various numerical/categorical columns in the data. Note that the describe() function only describes numerical columns by default. Sometimes, the describe() function will reveal outliers that indicate mistakes in the data. df . describe () Indexing the dataset \u00b6 We can access a particular column of DataFrame by indexing the object using the column name. For example: df [ \"band_gap\" ] Alternatively, we can access a particular row of a Dataframe using the iloc attribute. df . iloc [ 100 ] Filtering the dataset \u00b6 Pandas DataFrame objects make it very easy to filter the data based on a specific column. We can use the typical Python comparison operators (==, >, >=, <, etc) to filter numerical values. For example, let's find all entries where the cell volume is greater than 580. We do this by filtering on the volume column. Note that we first produce a boolean mask \u2013 a series of True and False depending on the comparison. We can then use the mask to filter the DataFrame . mask = df [ \"volume\" ] >= 580 df [ mask ] We can use this method of filtering to clean our dataset. For example, if we only wanted our dataset to include semiconductors (materials with a non-zero band gap), we can do this easily by filtering the band_gap column. mask = df [ \"band_gap\" ] > 0 semiconductor_df = df [ mask ] semiconductor_df Often, a dataset contains many additional columns that are not necessary for machine learning. Before we can train a model on the data, we need to remove any extraneous columns. We can remove whole columns from the dataset using the drop() function. This function can be used to drop both rows and columns. The function takes a list of items to drop. For columns, this is column names whereas for rows it is the row number. Finally, the axis option specifies whether the data to drop is columns ( 1 ) or rows ( 0 ). For example, to remove the nsites , space_group , e_electronic , and e_total columns, we can run: cleaned_df = df . drop ([ \"nsites\" , \"space_group\" , \"e_electronic\" , \"e_total\" ], axis = 1 ) Let's examine the cleaned DataFrame to see that the columns have been removed. cleaned_df . head () Generating new columns \u00b6 Pandas DataFrame objects also make it easy to perform simple calculations on the data. Think of this as using formulas in Excel spreadsheets. All fundamental Python math operators (such as +, -, /, and *) can be used. For example, the dielectric dataset contains the electronic contribution to the dielectric constant ($\\epsilon_\\mathrm{electronic}$, in the poly_electronic column) and the total (static) dielectric constant ($\\epsilon_\\mathrm{total}$, in the poly_total column). The ionic contribution to the dataset is given by: $$ \\epsilon_\\mathrm{ionic} = \\epsilon_\\mathrm{total} - \\epsilon_\\mathrm{electronic} $$ Below, we calculate the ionic contribution to the dielectric constant and store it in a new column called poly_ionic . This is as simple as assigning the data to the new column, even if the column doesn't already exist. df [ \"poly_ionic\" ] = df [ \"poly_total\" ] - df [ \"poly_electronic\" ] Let's check the new data was added correctly. df . head () Let's practice! \u00b6 In this section we showed how to: - download datasets - inspect a dataset - filter a dataset - add and remove columsn from a dataset Now, let's practice. You'll download a dataset, inspect it, and make sure it is ready to be used for machine learning.","title":"Unit 1 notes"},{"location":"08_ml_matminer/unit-1-notes/#materials-data-science-descriptors-and-machine-learning","text":"Welcome to the materials data science lesson. In this session, we will demonstrate how to use matminer , pandas and scikit-learn for machine learning materials properties. The lesson is split into three sections: 1. Data retrieval and basic analysis of pandas DataFrame objects. 2. Generating machine learnable descriptors. 3. Training, testing and visualizing machine learning methods with scikit-learn and FigRecipes . 4. AMM? Many more tutorials on how to use matminer (beyond the scope of this workshop) are available in the matminer_examples repository, available here .","title":"Materials data science: descriptors and machine learning"},{"location":"08_ml_matminer/unit-1-notes/#machine-learning-workflow","text":"Firstly, what does a typical machine learning workflow look like? The overall process can be summarized as: 1. Take raw inputs, such as a list of compositions, and an associated target property to learn. 2. Convert the raw inputs into descriptors or features that can be learned by machine learning algorithms. 3. Train a machine learning model on the data. 4. Plot and analyze the performance of the model. Typically, questions asked by a new practitioner in the field include: - Where do we get the raw data from? - How do we convert the raw data into learnable features? - How can we plot and interpret the results of a model? The `matminer` package has been developed to help make machine learning of materials properties easy and hassle free. The aim of matminer is to connect materials data with data mining algorithms and data visualization.","title":"Machine learning workflow"},{"location":"08_ml_matminer/unit-1-notes/#part-1-data-retrieval-and-filtering","text":"Matminer interfaces with many materials databases, including: - Materials Project - Citrine - AFLOW - Materials Data Facility (MDF) - Materials Platform for Data Science (MPDS) In addition, it also includes datasets from published literature. Matminer hosts a repository of 26 (and growing) datasets which comes from published and peer-reviewed machine learning investigations of materials properties or publications of high-throughput computing studies. In this section, we will show how to access and manipulate the datasets from the published literature. More information on accessing other materials databases are detailed in the matminer_examples repository. A list of the literature-based datasets can be printed using the get_available_datasets() function. This also prints information about what the dataset contains, such as the number of samples, the target properties, and how the data was obtained (e.g., via theory or experiment). from matminer.datasets import get_available_datasets get_available_datasets () Datasets can be loaded using the load_dataset() function and the database name. To save installation space, the datasets are not automatically downloaded when matminer is installed. Instead, the first time the dataset is loaded, it will be downloaded from the internet and stored in the matminer installation directory. Let's load the dielectric_constant dataset. It contains 1,056 structures with dielectric properties calculated with DFPT-PBE. from matminer.datasets import load_dataset df = load_dataset ( \"dielectric_constant\" )","title":"Part 1: Data retrieval and filtering"},{"location":"08_ml_matminer/unit-1-notes/#manipulating-and-examining-pandas-dataframe-objects","text":"The datasets are made available as pandas DataFrame objects. You can think of these as a type of \"spreadsheet\" object in Python. DataFrames have several useful methods you can use to explore and clean the data, some of which we'll explore below.","title":"Manipulating and examining pandas DataFrame objects"},{"location":"08_ml_matminer/unit-1-notes/#inspecting-the-dataset","text":"The head() function prints a summary of the first few rows of a data set. You can scroll across to see more columns. From this, it is easy to see the types of data available in in the dataset. df . head () Sometimes, if a dataset is very large, you will be unable to see all the available columns. Instead, you can see the full list of columns using the columns attribute: df . columns A pandas DataFrame includes a function called describe() that helps determine statistics for the various numerical/categorical columns in the data. Note that the describe() function only describes numerical columns by default. Sometimes, the describe() function will reveal outliers that indicate mistakes in the data. df . describe ()","title":"Inspecting the dataset"},{"location":"08_ml_matminer/unit-1-notes/#indexing-the-dataset","text":"We can access a particular column of DataFrame by indexing the object using the column name. For example: df [ \"band_gap\" ] Alternatively, we can access a particular row of a Dataframe using the iloc attribute. df . iloc [ 100 ]","title":"Indexing the dataset"},{"location":"08_ml_matminer/unit-1-notes/#filtering-the-dataset","text":"Pandas DataFrame objects make it very easy to filter the data based on a specific column. We can use the typical Python comparison operators (==, >, >=, <, etc) to filter numerical values. For example, let's find all entries where the cell volume is greater than 580. We do this by filtering on the volume column. Note that we first produce a boolean mask \u2013 a series of True and False depending on the comparison. We can then use the mask to filter the DataFrame . mask = df [ \"volume\" ] >= 580 df [ mask ] We can use this method of filtering to clean our dataset. For example, if we only wanted our dataset to include semiconductors (materials with a non-zero band gap), we can do this easily by filtering the band_gap column. mask = df [ \"band_gap\" ] > 0 semiconductor_df = df [ mask ] semiconductor_df Often, a dataset contains many additional columns that are not necessary for machine learning. Before we can train a model on the data, we need to remove any extraneous columns. We can remove whole columns from the dataset using the drop() function. This function can be used to drop both rows and columns. The function takes a list of items to drop. For columns, this is column names whereas for rows it is the row number. Finally, the axis option specifies whether the data to drop is columns ( 1 ) or rows ( 0 ). For example, to remove the nsites , space_group , e_electronic , and e_total columns, we can run: cleaned_df = df . drop ([ \"nsites\" , \"space_group\" , \"e_electronic\" , \"e_total\" ], axis = 1 ) Let's examine the cleaned DataFrame to see that the columns have been removed. cleaned_df . head ()","title":"Filtering the dataset"},{"location":"08_ml_matminer/unit-1-notes/#generating-new-columns","text":"Pandas DataFrame objects also make it easy to perform simple calculations on the data. Think of this as using formulas in Excel spreadsheets. All fundamental Python math operators (such as +, -, /, and *) can be used. For example, the dielectric dataset contains the electronic contribution to the dielectric constant ($\\epsilon_\\mathrm{electronic}$, in the poly_electronic column) and the total (static) dielectric constant ($\\epsilon_\\mathrm{total}$, in the poly_total column). The ionic contribution to the dataset is given by: $$ \\epsilon_\\mathrm{ionic} = \\epsilon_\\mathrm{total} - \\epsilon_\\mathrm{electronic} $$ Below, we calculate the ionic contribution to the dielectric constant and store it in a new column called poly_ionic . This is as simple as assigning the data to the new column, even if the column doesn't already exist. df [ \"poly_ionic\" ] = df [ \"poly_total\" ] - df [ \"poly_electronic\" ] Let's check the new data was added correctly. df . head ()","title":"Generating new columns"},{"location":"08_ml_matminer/unit-1-notes/#lets-practice","text":"In this section we showed how to: - download datasets - inspect a dataset - filter a dataset - add and remove columsn from a dataset Now, let's practice. You'll download a dataset, inspect it, and make sure it is ready to be used for machine learning.","title":"Let's practice!"},{"location":"08_ml_matminer/unit-2-exercises-answers/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Generating materials descriptors \u2013 Exercises \u00b6 In these exercises, we'll load a cleaned dataframe, decorate it with multiple descriptors, and prepare it to be used for machine learning. Before starting, we need to use matminer's load_dataframe_from_json() function to load a cleaned version of the elastic_tensor_2015 dataset. We will use this dataset for all the exercises. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"elastic_tensor_2015_cleaned.json\" )) df . head () Exercise 1: Convert formulas to pymatgen Compositions \u00b6 Use matminer's StrToComposition conversion featurizer to first convert the formula column of the dataframe to pymatgen Composition s. This is necessary because matminer's Composition featurizers need pymatgen compositions as input. from matminer.featurizers.conversions import StrToComposition stc = StrToComposition () # Complete exercise below df = stc . featurize_dataframe ( df , \"formula\" ) df . head () Exercise 2: Add composition features \u00b6 Now add ElementFraction features by featurizing the composition column. from matminer.featurizers.composition import ElementFraction ep = ElementFraction () # Complete exercise below df = ep . featurize_dataframe ( df , \"composition\" ) df . head () Exercise 3: Add structure features \u00b6 Finally, structure features using the DensityFeatures featurizer on the structure column. from matminer.featurizers.structure import DensityFeatures de = DensityFeatures () # Complete exercise below df = de . featurize_dataframe ( df , \"structure\" ) df . head () Great! We've generated our features. Onto the next section.","title":"Unit 2 exercises answers"},{"location":"08_ml_matminer/unit-2-exercises-answers/#generating-materials-descriptors-exercises","text":"In these exercises, we'll load a cleaned dataframe, decorate it with multiple descriptors, and prepare it to be used for machine learning. Before starting, we need to use matminer's load_dataframe_from_json() function to load a cleaned version of the elastic_tensor_2015 dataset. We will use this dataset for all the exercises. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"elastic_tensor_2015_cleaned.json\" )) df . head ()","title":"Generating materials descriptors \u2013 Exercises"},{"location":"08_ml_matminer/unit-2-exercises-answers/#exercise-1-convert-formulas-to-pymatgen-compositions","text":"Use matminer's StrToComposition conversion featurizer to first convert the formula column of the dataframe to pymatgen Composition s. This is necessary because matminer's Composition featurizers need pymatgen compositions as input. from matminer.featurizers.conversions import StrToComposition stc = StrToComposition () # Complete exercise below df = stc . featurize_dataframe ( df , \"formula\" ) df . head ()","title":"Exercise 1: Convert formulas to pymatgen Compositions"},{"location":"08_ml_matminer/unit-2-exercises-answers/#exercise-2-add-composition-features","text":"Now add ElementFraction features by featurizing the composition column. from matminer.featurizers.composition import ElementFraction ep = ElementFraction () # Complete exercise below df = ep . featurize_dataframe ( df , \"composition\" ) df . head ()","title":"Exercise 2: Add composition features"},{"location":"08_ml_matminer/unit-2-exercises-answers/#exercise-3-add-structure-features","text":"Finally, structure features using the DensityFeatures featurizer on the structure column. from matminer.featurizers.structure import DensityFeatures de = DensityFeatures () # Complete exercise below df = de . featurize_dataframe ( df , \"structure\" ) df . head () Great! We've generated our features. Onto the next section.","title":"Exercise 3: Add structure features"},{"location":"08_ml_matminer/unit-2-exercises/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Generating materials descriptors \u2013 Exercises \u00b6 In these exercises, we'll load a cleaned dataframe, decorate it with multiple descriptors, and prepare it to be used for machine learning. Before starting, we need to use matminer's load_dataframe_from_json() function to load a cleaned version of the elastic_tensor_2015 dataset. We will use this dataset for all the exercises. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"elastic_tensor_2015_cleaned.json\" )) df . head () Exercise 1: Convert formulas to pymatgen Compositions \u00b6 Use matminer's StrToComposition conversion featurizer to first convert the formula column of the dataframe to pymatgen Composition s. This is necessary because matminer's Composition featurizers need pymatgen compositions as input. from matminer.featurizers.conversions import StrToComposition stc = StrToComposition () # Fill in the blanks below df = stc . featurize_dataframe ( __ , ___ ) Exercise 2: Add composition features \u00b6 Now add ElementFraction features by featurizing the composition column. from matminer.featurizers.composition import ElementFraction ep = ElementFraction () # Fill in the blanks below df = ep . featurize_dataframe ( __ , ___ ) Exercise 3: Add structure features \u00b6 Finally, structure features using the DensityFeatures featurizer on the structure column. from matminer.featurizers.structure import DensityFeatures de = DensityFeatures () # Fill in the blanks below df = de . featurize_dataframe ( __ , ___ ) Great! We've generated our features. Onto the next section.","title":"Unit 2 exercises"},{"location":"08_ml_matminer/unit-2-exercises/#generating-materials-descriptors-exercises","text":"In these exercises, we'll load a cleaned dataframe, decorate it with multiple descriptors, and prepare it to be used for machine learning. Before starting, we need to use matminer's load_dataframe_from_json() function to load a cleaned version of the elastic_tensor_2015 dataset. We will use this dataset for all the exercises. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"elastic_tensor_2015_cleaned.json\" )) df . head ()","title":"Generating materials descriptors \u2013 Exercises"},{"location":"08_ml_matminer/unit-2-exercises/#exercise-1-convert-formulas-to-pymatgen-compositions","text":"Use matminer's StrToComposition conversion featurizer to first convert the formula column of the dataframe to pymatgen Composition s. This is necessary because matminer's Composition featurizers need pymatgen compositions as input. from matminer.featurizers.conversions import StrToComposition stc = StrToComposition () # Fill in the blanks below df = stc . featurize_dataframe ( __ , ___ )","title":"Exercise 1: Convert formulas to pymatgen Compositions"},{"location":"08_ml_matminer/unit-2-exercises/#exercise-2-add-composition-features","text":"Now add ElementFraction features by featurizing the composition column. from matminer.featurizers.composition import ElementFraction ep = ElementFraction () # Fill in the blanks below df = ep . featurize_dataframe ( __ , ___ )","title":"Exercise 2: Add composition features"},{"location":"08_ml_matminer/unit-2-exercises/#exercise-3-add-structure-features","text":"Finally, structure features using the DensityFeatures featurizer on the structure column. from matminer.featurizers.structure import DensityFeatures de = DensityFeatures () # Fill in the blanks below df = de . featurize_dataframe ( __ , ___ ) Great! We've generated our features. Onto the next section.","title":"Exercise 3: Add structure features"},{"location":"08_ml_matminer/unit-2-notes-blank/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Part 2: Generating descriptors for machine learning \u00b6 In this lesson, we will learn a bit about how to generate machine-learning descriptors from materials objects in pymatgen. First, we'll generate some descriptors with matminer's \"featurizers\" classes. Next, we'll use some of what we learned about dataframes in the previous section to examine our descriptors and prepare them for input to machine learning models. ### Featurizers transform materials primitives into machine-learnable features The general idea of featurizers is that they accept a materials primitive (e.g., pymatgen Composition) and output a vector. For example: \\begin{align} f(\\mathrm{Fe}_2\\mathrm{O}_3) \\rightarrow [1.5, 7.8, 9.1, 0.09] \\end{align} #### Matminer contains featurizers for the following pymatgen objects: * Composition * Crystal structure * Crystal sites * Bandstructure * Density of states #### Depending on the featurizer, the features returned may be: * numerical, categorical, or mixed vectors * matrices * other pymatgen objects (for further processing) ### Featurizers play nice with dataframes Since most of the time we are working with pandas dataframes, all featurizers work natively with pandas dataframes. We'll provide examples of this later in the lesson. ### Featurizers present in matminer Matminer hosts over 60 featurizers, most of which are implemented from methods published in peer reviewed papers. You can find a full list of featurizers on the [matminer website](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html). All featurizers have parallelization and convenient error tolerance built into their core methods. In this lesson, we'll go over the main methods present in all featurizers. By the end of this unit, you will be able to generate descriptors for a wide range of materials informatics problems using one common software interface. The featurize method and basics \u00b6 The core method of any matminer is \"featurize\". This method accepts a materials object and returns a machine learning vector or matrix. Let's see an example on a pymatgen composition: from pymatgen import Composition fe2o3 = Composition ( \"Fe2O3\" ) As a trivial example, we'll get the element fractions with the ElementFraction featurizer. from matminer.featurizers.composition import ElementFraction Now we can featurize our composition. We've managed to generate features for learning, but what do they mean? One way to check is by reading the Features section in the documentation of any featurizer... but a much easier way is to use the feature_labels() method. We now see the labels in the order that we generated the features. Featurizing dataframes \u00b6 We just generated some descriptors and their labels from an individual sample but most of the time our data is in pandas dataframes. Fortunately, matminer featurizers implement a featurize_dataframe() method which interacts natively with dataframes. Let's grab a new dataset from matminer and use our ElementFraction featurizer on it. First, we download a dataset as we did in the previous unit. In this example, we'll download a dataset of super hard materials. from matminer.datasets.dataset_retrieval import load_dataset df = load_dataset ( \"brgoch_superhard_training\" ) df . head () Next, we can use the featurize_dataframe() method (implemented by all featurizers) to apply ElementFraction to all of our data at once. The only required arguments are the dataframe as input and the input column name (in this case it is composition ). featurize_dataframe() is parallelized by default using multiprocessing. If we look at the database we can see our new feature columns. Structure Featurizers \u00b6 We can use the same syntax for other kinds of featurizers. Let's now assign descriptors to a structure. We do this with the same syntax as the composition featurizers. First, let's load a dataset containing structures. df = load_dataset ( \"phonon_dielectric_mp\" ) df . head () Let's calculate some basic density features of these structures using DensityFeatures . from matminer.featurizers.structure import DensityFeatures These are the features we will get. Now we use featurize_dataframe() to generate these features for all the samples in the dataframe. Since we are using the structures as input to the featurizer, we select the \"structure\" column. Let's examine the dataframe and see the structural features. Conversion Featurizers \u00b6 In addition to Bandstructure/DOS/Structure/Composition featurizers, matminer also provides a featurizer interface for converting between pymatgen objects (e.g., assinging oxidation states to compositions) in a fault-tolerant fashion. These featurizers are found in matminer.featurizers.conversion and work with the same featurize / featurize_dataframe etc. syntax as the other featurizers. The dataset we loaded previously only contains a formula column with string objects. To convert this data into a composition column containing pymatgen Composition objects, we can use the StrToComposition conversion featurizer on the formula column. from matminer.featurizers.conversions import StrToComposition We can see a new composition column has been added to the dataframe. Advanced capabilities \u00b6 There are powerful functionalities of Featurizers which are worth quickly mentioning before we go practice (and many more not mentioned here). Dealing with Errors Often, data is messy and certain featurizers will encounter errors. Set ignore_errors=True in featurize_dataframe() to skip errors; if you'd like to see the errors returned in an additional column, also set return_errors=True . Citing the authors Many featurizers are implemented using methods found in peer reviewed studies. Please cite these original works using the citations() method, which returns the BibTex-formatted references in a Python list. For example: Let's practice! \u00b6 Now, let's practice. You'll pick up where you left off from the last lesson, add some descriptors using the techiques described here, and prepare your data for the final unit.","title":"Unit 2 notes blank"},{"location":"08_ml_matminer/unit-2-notes-blank/#part-2-generating-descriptors-for-machine-learning","text":"In this lesson, we will learn a bit about how to generate machine-learning descriptors from materials objects in pymatgen. First, we'll generate some descriptors with matminer's \"featurizers\" classes. Next, we'll use some of what we learned about dataframes in the previous section to examine our descriptors and prepare them for input to machine learning models. ### Featurizers transform materials primitives into machine-learnable features The general idea of featurizers is that they accept a materials primitive (e.g., pymatgen Composition) and output a vector. For example: \\begin{align} f(\\mathrm{Fe}_2\\mathrm{O}_3) \\rightarrow [1.5, 7.8, 9.1, 0.09] \\end{align} #### Matminer contains featurizers for the following pymatgen objects: * Composition * Crystal structure * Crystal sites * Bandstructure * Density of states #### Depending on the featurizer, the features returned may be: * numerical, categorical, or mixed vectors * matrices * other pymatgen objects (for further processing) ### Featurizers play nice with dataframes Since most of the time we are working with pandas dataframes, all featurizers work natively with pandas dataframes. We'll provide examples of this later in the lesson. ### Featurizers present in matminer Matminer hosts over 60 featurizers, most of which are implemented from methods published in peer reviewed papers. You can find a full list of featurizers on the [matminer website](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html). All featurizers have parallelization and convenient error tolerance built into their core methods. In this lesson, we'll go over the main methods present in all featurizers. By the end of this unit, you will be able to generate descriptors for a wide range of materials informatics problems using one common software interface.","title":"Part 2: Generating descriptors for machine learning"},{"location":"08_ml_matminer/unit-2-notes-blank/#the-featurize-method-and-basics","text":"The core method of any matminer is \"featurize\". This method accepts a materials object and returns a machine learning vector or matrix. Let's see an example on a pymatgen composition: from pymatgen import Composition fe2o3 = Composition ( \"Fe2O3\" ) As a trivial example, we'll get the element fractions with the ElementFraction featurizer. from matminer.featurizers.composition import ElementFraction Now we can featurize our composition. We've managed to generate features for learning, but what do they mean? One way to check is by reading the Features section in the documentation of any featurizer... but a much easier way is to use the feature_labels() method. We now see the labels in the order that we generated the features.","title":"The featurize method and basics"},{"location":"08_ml_matminer/unit-2-notes-blank/#featurizing-dataframes","text":"We just generated some descriptors and their labels from an individual sample but most of the time our data is in pandas dataframes. Fortunately, matminer featurizers implement a featurize_dataframe() method which interacts natively with dataframes. Let's grab a new dataset from matminer and use our ElementFraction featurizer on it. First, we download a dataset as we did in the previous unit. In this example, we'll download a dataset of super hard materials. from matminer.datasets.dataset_retrieval import load_dataset df = load_dataset ( \"brgoch_superhard_training\" ) df . head () Next, we can use the featurize_dataframe() method (implemented by all featurizers) to apply ElementFraction to all of our data at once. The only required arguments are the dataframe as input and the input column name (in this case it is composition ). featurize_dataframe() is parallelized by default using multiprocessing. If we look at the database we can see our new feature columns.","title":"Featurizing  dataframes"},{"location":"08_ml_matminer/unit-2-notes-blank/#structure-featurizers","text":"We can use the same syntax for other kinds of featurizers. Let's now assign descriptors to a structure. We do this with the same syntax as the composition featurizers. First, let's load a dataset containing structures. df = load_dataset ( \"phonon_dielectric_mp\" ) df . head () Let's calculate some basic density features of these structures using DensityFeatures . from matminer.featurizers.structure import DensityFeatures These are the features we will get. Now we use featurize_dataframe() to generate these features for all the samples in the dataframe. Since we are using the structures as input to the featurizer, we select the \"structure\" column. Let's examine the dataframe and see the structural features.","title":"Structure Featurizers"},{"location":"08_ml_matminer/unit-2-notes-blank/#conversion-featurizers","text":"In addition to Bandstructure/DOS/Structure/Composition featurizers, matminer also provides a featurizer interface for converting between pymatgen objects (e.g., assinging oxidation states to compositions) in a fault-tolerant fashion. These featurizers are found in matminer.featurizers.conversion and work with the same featurize / featurize_dataframe etc. syntax as the other featurizers. The dataset we loaded previously only contains a formula column with string objects. To convert this data into a composition column containing pymatgen Composition objects, we can use the StrToComposition conversion featurizer on the formula column. from matminer.featurizers.conversions import StrToComposition We can see a new composition column has been added to the dataframe.","title":"Conversion Featurizers"},{"location":"08_ml_matminer/unit-2-notes-blank/#advanced-capabilities","text":"There are powerful functionalities of Featurizers which are worth quickly mentioning before we go practice (and many more not mentioned here). Dealing with Errors Often, data is messy and certain featurizers will encounter errors. Set ignore_errors=True in featurize_dataframe() to skip errors; if you'd like to see the errors returned in an additional column, also set return_errors=True . Citing the authors Many featurizers are implemented using methods found in peer reviewed studies. Please cite these original works using the citations() method, which returns the BibTex-formatted references in a Python list. For example:","title":"Advanced capabilities"},{"location":"08_ml_matminer/unit-2-notes-blank/#lets-practice","text":"Now, let's practice. You'll pick up where you left off from the last lesson, add some descriptors using the techiques described here, and prepare your data for the final unit.","title":"Let's practice!"},{"location":"08_ml_matminer/unit-2-notes/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Part 2: Generating descriptors for machine learning \u00b6 In this lesson, we will learn a bit about how to generate machine-learning descriptors from materials objects in pymatgen. First, we'll generate some descriptors with matminer's \"featurizers\" classes. Next, we'll use some of what we learned about dataframes in the previous section to examine our descriptors and prepare them for input to machine learning models. ### Featurizers transform materials primitives into machine-learnable features The general idea of featurizers is that they accept a materials primitive (e.g., pymatgen Composition) and output a vector. For example: \\begin{align} f(\\mathrm{Fe}_2\\mathrm{O}_3) \\rightarrow [1.5, 7.8, 9.1, 0.09] \\end{align} #### Matminer contains featurizers for the following pymatgen objects: * Composition * Crystal structure * Crystal sites * Bandstructure * Density of states #### Depending on the featurizer, the features returned may be: * numerical, categorical, or mixed vectors * matrices * other pymatgen objects (for further processing) ### Featurizers play nice with dataframes Since most of the time we are working with pandas dataframes, all featurizers work natively with pandas dataframes. We'll provide examples of this later in the lesson. ### Featurizers present in matminer Matminer hosts over 60 featurizers, most of which are implemented from methods published in peer reviewed papers. You can find a full list of featurizers on the [matminer website](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html). All featurizers have parallelization and convenient error tolerance built into their core methods. In this lesson, we'll go over the main methods present in all featurizers. By the end of this unit, you will be able to generate descriptors for a wide range of materials informatics problems using one common software interface. The featurize method and basics \u00b6 The core method of any matminer is \"featurize\". This method accepts a materials object and returns a machine learning vector or matrix. Let's see an example on a pymatgen composition: from pymatgen import Composition fe2o3 = Composition ( \"Fe2O3\" ) As a trivial example, we'll get the element fractions with the ElementFraction featurizer. from matminer.featurizers.composition import ElementFraction ef = ElementFraction () Now we can featurize our composition. element_fractions = ef . featurize ( fe2o3 ) print ( element_fractions ) We've managed to generate features for learning, but what do they mean? One way to check is by reading the Features section in the documentation of any featurizer... but a much easier way is to use the feature_labels() method. element_fraction_labels = ef . feature_labels () print ( element_fraction_labels ) We now see the labels in the order that we generated the features. print ( element_fraction_labels [ 7 ], element_fractions [ 7 ]) print ( element_fraction_labels [ 25 ], element_fractions [ 25 ]) Featurizing dataframes \u00b6 We just generated some descriptors and their labels from an individual sample but most of the time our data is in pandas dataframes. Fortunately, matminer featurizers implement a featurize_dataframe() method which interacts natively with dataframes. Let's grab a new dataset from matminer and use our ElementFraction featurizer on it. First, we download a dataset as we did in the previous unit. In this example, we'll download a dataset of super hard materials. from matminer.datasets.dataset_retrieval import load_dataset df = load_dataset ( \"brgoch_superhard_training\" ) df . head () Next, we can use the featurize_dataframe() method (implemented by all featurizers) to apply ElementFraction to all of our data at once. The only required arguments are the dataframe as input and the input column name (in this case it is composition ). featurize_dataframe() is parallelized by default using multiprocessing. df = ef . featurize_dataframe ( df , \"composition\" ) If we look at the database we can see our new feature columns. df . head () Structure Featurizers \u00b6 We can use the same syntax for other kinds of featurizers. Let's now assign descriptors to a structure. We do this with the same syntax as the composition featurizers. First, let's load a dataset containing structures. df = load_dataset ( \"phonon_dielectric_mp\" ) df . head () Let's calculate some basic density features of these structures using DensityFeatures . from matminer.featurizers.structure import DensityFeatures densityf = DensityFeatures () densityf . feature_labels () These are the features we will get. Now we use featurize_dataframe() to generate these features for all the samples in the dataframe. Since we are using the structures as input to the featurizer, we select the \"structure\" column. df = densityf . featurize_dataframe ( df , \"structure\" ) Let's examine the dataframe and see the structural features. Conversion Featurizers \u00b6 In addition to Bandstructure/DOS/Structure/Composition featurizers, matminer also provides a featurizer interface for converting between pymatgen objects (e.g., assinging oxidation states to compositions) in a fault-tolerant fashion. These featurizers are found in matminer.featurizers.conversion and work with the same featurize / featurize_dataframe etc. syntax as the other featurizers. The dataset we loaded previously only contains a formula column with string objects. To convert this data into a composition column containing pymatgen Composition objects, we can use the StrToComposition conversion featurizer on the formula column. from matminer.featurizers.conversions import StrToComposition stc = StrToComposition () df = stc . featurize_dataframe ( df , \"formula\" ) We can see a new composition column has been added to the dataframe. df . head () Advanced capabilities \u00b6 There are powerful functionalities of Featurizers which are worth quickly mentioning before we go practice (and many more not mentioned here). Dealing with Errors Often, data is messy and certain featurizers will encounter errors. Set ignore_errors=True in featurize_dataframe() to skip errors; if you'd like to see the errors returned in an additional column, also set return_errors=True . Citing the authors Many featurizers are implemented using methods found in peer reviewed studies. Please cite these original works using the citations() method, which returns the BibTex-formatted references in a Python list. For example: Let's practice! \u00b6 Now, let's practice. You'll pick up where you left off from the last lesson, add some descriptors using the techiques described here, and prepare your data for the final unit.","title":"Unit 2 notes"},{"location":"08_ml_matminer/unit-2-notes/#part-2-generating-descriptors-for-machine-learning","text":"In this lesson, we will learn a bit about how to generate machine-learning descriptors from materials objects in pymatgen. First, we'll generate some descriptors with matminer's \"featurizers\" classes. Next, we'll use some of what we learned about dataframes in the previous section to examine our descriptors and prepare them for input to machine learning models. ### Featurizers transform materials primitives into machine-learnable features The general idea of featurizers is that they accept a materials primitive (e.g., pymatgen Composition) and output a vector. For example: \\begin{align} f(\\mathrm{Fe}_2\\mathrm{O}_3) \\rightarrow [1.5, 7.8, 9.1, 0.09] \\end{align} #### Matminer contains featurizers for the following pymatgen objects: * Composition * Crystal structure * Crystal sites * Bandstructure * Density of states #### Depending on the featurizer, the features returned may be: * numerical, categorical, or mixed vectors * matrices * other pymatgen objects (for further processing) ### Featurizers play nice with dataframes Since most of the time we are working with pandas dataframes, all featurizers work natively with pandas dataframes. We'll provide examples of this later in the lesson. ### Featurizers present in matminer Matminer hosts over 60 featurizers, most of which are implemented from methods published in peer reviewed papers. You can find a full list of featurizers on the [matminer website](https://hackingmaterials.lbl.gov/matminer/featurizer_summary.html). All featurizers have parallelization and convenient error tolerance built into their core methods. In this lesson, we'll go over the main methods present in all featurizers. By the end of this unit, you will be able to generate descriptors for a wide range of materials informatics problems using one common software interface.","title":"Part 2: Generating descriptors for machine learning"},{"location":"08_ml_matminer/unit-2-notes/#the-featurize-method-and-basics","text":"The core method of any matminer is \"featurize\". This method accepts a materials object and returns a machine learning vector or matrix. Let's see an example on a pymatgen composition: from pymatgen import Composition fe2o3 = Composition ( \"Fe2O3\" ) As a trivial example, we'll get the element fractions with the ElementFraction featurizer. from matminer.featurizers.composition import ElementFraction ef = ElementFraction () Now we can featurize our composition. element_fractions = ef . featurize ( fe2o3 ) print ( element_fractions ) We've managed to generate features for learning, but what do they mean? One way to check is by reading the Features section in the documentation of any featurizer... but a much easier way is to use the feature_labels() method. element_fraction_labels = ef . feature_labels () print ( element_fraction_labels ) We now see the labels in the order that we generated the features. print ( element_fraction_labels [ 7 ], element_fractions [ 7 ]) print ( element_fraction_labels [ 25 ], element_fractions [ 25 ])","title":"The featurize method and basics"},{"location":"08_ml_matminer/unit-2-notes/#featurizing-dataframes","text":"We just generated some descriptors and their labels from an individual sample but most of the time our data is in pandas dataframes. Fortunately, matminer featurizers implement a featurize_dataframe() method which interacts natively with dataframes. Let's grab a new dataset from matminer and use our ElementFraction featurizer on it. First, we download a dataset as we did in the previous unit. In this example, we'll download a dataset of super hard materials. from matminer.datasets.dataset_retrieval import load_dataset df = load_dataset ( \"brgoch_superhard_training\" ) df . head () Next, we can use the featurize_dataframe() method (implemented by all featurizers) to apply ElementFraction to all of our data at once. The only required arguments are the dataframe as input and the input column name (in this case it is composition ). featurize_dataframe() is parallelized by default using multiprocessing. df = ef . featurize_dataframe ( df , \"composition\" ) If we look at the database we can see our new feature columns. df . head ()","title":"Featurizing  dataframes"},{"location":"08_ml_matminer/unit-2-notes/#structure-featurizers","text":"We can use the same syntax for other kinds of featurizers. Let's now assign descriptors to a structure. We do this with the same syntax as the composition featurizers. First, let's load a dataset containing structures. df = load_dataset ( \"phonon_dielectric_mp\" ) df . head () Let's calculate some basic density features of these structures using DensityFeatures . from matminer.featurizers.structure import DensityFeatures densityf = DensityFeatures () densityf . feature_labels () These are the features we will get. Now we use featurize_dataframe() to generate these features for all the samples in the dataframe. Since we are using the structures as input to the featurizer, we select the \"structure\" column. df = densityf . featurize_dataframe ( df , \"structure\" ) Let's examine the dataframe and see the structural features.","title":"Structure Featurizers"},{"location":"08_ml_matminer/unit-2-notes/#conversion-featurizers","text":"In addition to Bandstructure/DOS/Structure/Composition featurizers, matminer also provides a featurizer interface for converting between pymatgen objects (e.g., assinging oxidation states to compositions) in a fault-tolerant fashion. These featurizers are found in matminer.featurizers.conversion and work with the same featurize / featurize_dataframe etc. syntax as the other featurizers. The dataset we loaded previously only contains a formula column with string objects. To convert this data into a composition column containing pymatgen Composition objects, we can use the StrToComposition conversion featurizer on the formula column. from matminer.featurizers.conversions import StrToComposition stc = StrToComposition () df = stc . featurize_dataframe ( df , \"formula\" ) We can see a new composition column has been added to the dataframe. df . head ()","title":"Conversion Featurizers"},{"location":"08_ml_matminer/unit-2-notes/#advanced-capabilities","text":"There are powerful functionalities of Featurizers which are worth quickly mentioning before we go practice (and many more not mentioned here). Dealing with Errors Often, data is messy and certain featurizers will encounter errors. Set ignore_errors=True in featurize_dataframe() to skip errors; if you'd like to see the errors returned in an additional column, also set return_errors=True . Citing the authors Many featurizers are implemented using methods found in peer reviewed studies. Please cite these original works using the citations() method, which returns the BibTex-formatted references in a Python list. For example:","title":"Advanced capabilities"},{"location":"08_ml_matminer/unit-2-notes/#lets-practice","text":"Now, let's practice. You'll pick up where you left off from the last lesson, add some descriptors using the techiques described here, and prepare your data for the final unit.","title":"Let's practice!"},{"location":"08_ml_matminer/unit-3-exercies-answers/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Machine learning models \u2013 Exercises \u00b6 In these exercises, we'll load a cleaned and featurized dataframe then use scikit-learn to predict materials properties. Before starting, we need to use matminer's load_dataframe_from_json() function to load a cleaned and featurized version of the dielectric_constant dataset. We will use this dataset for all the exercises. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"dielectric_constant_featurized.json\" )) df . head () Exercise 1: Split dataset in target property and features \u00b6 You first need to partition the data into the target property and features used for learning. For this dataset, the target property is contained in the total_dielectric column. The features are all other columns, except structure , and composition . The target property data should be stored in the y variable. The set of features used for learning should be stored in the X variable. Hint remember to exclude the target property from the feature set. # Fill in the blanks y = df [ \"total_dielectric\" ] . values X = df . drop ([ \"structure\" , \"composition\" , \"total_dielectric\" ], axis = 1 ) Exercise 2: Train a random forest model on the dataset \u00b6 Train a random forest model with 150 estimators on the dataset. Next, use the model to get predictions for all samples and store them to the y_pred variable. from sklearn.ensemble import RandomForestRegressor # Fill in the blanks below rf = RandomForestRegressor ( n_estimators = 150 ) rf . fit ( X , y ) y_pred = rf . predict ( X ) To see how well your model is performing, run the next cell. import numpy as np from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y , y_pred ) print ( 'training RMSE = {:.3f} ' . format ( np . sqrt ( mse ))) Exercise 3: Evaluate the model using cross validation \u00b6 Evaluate your random forest model using cross validation with 5 splits. This will give a more realistic idea of how well your model will perform in practice. from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # Fill in the blanks below kfold = KFold ( n_splits = 5 ) scores = cross_val_score ( rf , X , y , scoring = 'neg_mean_squared_error' , cv = kfold ) The final cross validation score can be printed by running the cell below. rmse_scores = [ np . sqrt ( abs ( s )) for s in scores ] print ( 'Mean RMSE: {:.3f} ' . format ( np . mean ( rmse_scores )))","title":"Unit 3 exercies answers"},{"location":"08_ml_matminer/unit-3-exercies-answers/#machine-learning-models-exercises","text":"In these exercises, we'll load a cleaned and featurized dataframe then use scikit-learn to predict materials properties. Before starting, we need to use matminer's load_dataframe_from_json() function to load a cleaned and featurized version of the dielectric_constant dataset. We will use this dataset for all the exercises. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"dielectric_constant_featurized.json\" )) df . head ()","title":"Machine learning models \u2013 Exercises"},{"location":"08_ml_matminer/unit-3-exercies-answers/#exercise-1-split-dataset-in-target-property-and-features","text":"You first need to partition the data into the target property and features used for learning. For this dataset, the target property is contained in the total_dielectric column. The features are all other columns, except structure , and composition . The target property data should be stored in the y variable. The set of features used for learning should be stored in the X variable. Hint remember to exclude the target property from the feature set. # Fill in the blanks y = df [ \"total_dielectric\" ] . values X = df . drop ([ \"structure\" , \"composition\" , \"total_dielectric\" ], axis = 1 )","title":"Exercise 1: Split dataset in target property and features"},{"location":"08_ml_matminer/unit-3-exercies-answers/#exercise-2-train-a-random-forest-model-on-the-dataset","text":"Train a random forest model with 150 estimators on the dataset. Next, use the model to get predictions for all samples and store them to the y_pred variable. from sklearn.ensemble import RandomForestRegressor # Fill in the blanks below rf = RandomForestRegressor ( n_estimators = 150 ) rf . fit ( X , y ) y_pred = rf . predict ( X ) To see how well your model is performing, run the next cell. import numpy as np from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y , y_pred ) print ( 'training RMSE = {:.3f} ' . format ( np . sqrt ( mse )))","title":"Exercise 2: Train a random forest model on the dataset"},{"location":"08_ml_matminer/unit-3-exercies-answers/#exercise-3-evaluate-the-model-using-cross-validation","text":"Evaluate your random forest model using cross validation with 5 splits. This will give a more realistic idea of how well your model will perform in practice. from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # Fill in the blanks below kfold = KFold ( n_splits = 5 ) scores = cross_val_score ( rf , X , y , scoring = 'neg_mean_squared_error' , cv = kfold ) The final cross validation score can be printed by running the cell below. rmse_scores = [ np . sqrt ( abs ( s )) for s in scores ] print ( 'Mean RMSE: {:.3f} ' . format ( np . mean ( rmse_scores )))","title":"Exercise 3: Evaluate the model using cross validation"},{"location":"08_ml_matminer/unit-3-exercies/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Machine learning models \u2013 Exercises \u00b6 In these exercises, we'll load a cleaned and featurized dataframe then use scikit-learn to predict materials properties. Before starting, we need to use matminer's load_dataframe_from_json() function to load a cleaned and featurized version of the dielectric_constant dataset. We will use this dataset for all the exercises. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"dielectric_constant_featurized.json\" )) df . head () Exercise 1: Split dataset in target property and features \u00b6 You first need to partition the data into the target property and features used for learning. For this dataset, the target property is contained in the total_dielectric column. The features are all other columns, except structure , and composition . The target property data should be stored in the y variable. The set of features used for learning should be stored in the X variable. Hint remember to exclude the target property from the feature set. # Fill in the blanks y = df [ _____ ] . values X = df . drop ( _____ , axis = 1 ) Exercise 2: Train a random forest model on the dataset \u00b6 Train a random forest model with 150 estimators on the dataset. Next, use the model to get predictions for all samples and store them to the y_pred variable. from sklearn.ensemble import RandomForestRegressor # Fill in the blanks below rf = RandomForestRegressor ( ____ ) rf . fit ( __ , __ ) y_pred = rf . predict ( ___ ) To see how well your model is performing, run the next cell. import numpy as np from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y , y_pred ) print ( 'training RMSE = {:.3f} ' . format ( np . sqrt ( mse ))) Exercise 3: Evaluate the model using cross validation \u00b6 Evaluate your random forest model using cross validation with 5 splits. This will give a more realistic idea of how well your model will perform in practice. from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # Fill in the blanks below kfold = KFold ( ___ ) scores = cross_val_score ( __ , __ , __ , scoring = 'neg_mean_squared_error' , cv = ___ ) The final cross validation score can be printed by running the cell below. rmse_scores = [ np . sqrt ( abs ( s )) for s in scores ] print ( 'Mean RMSE: {:.3f} ' . format ( np . mean ( rmse_scores )))","title":"Unit 3 exercies"},{"location":"08_ml_matminer/unit-3-exercies/#machine-learning-models-exercises","text":"In these exercises, we'll load a cleaned and featurized dataframe then use scikit-learn to predict materials properties. Before starting, we need to use matminer's load_dataframe_from_json() function to load a cleaned and featurized version of the dielectric_constant dataset. We will use this dataset for all the exercises. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"dielectric_constant_featurized.json\" )) df . head ()","title":"Machine learning models \u2013 Exercises"},{"location":"08_ml_matminer/unit-3-exercies/#exercise-1-split-dataset-in-target-property-and-features","text":"You first need to partition the data into the target property and features used for learning. For this dataset, the target property is contained in the total_dielectric column. The features are all other columns, except structure , and composition . The target property data should be stored in the y variable. The set of features used for learning should be stored in the X variable. Hint remember to exclude the target property from the feature set. # Fill in the blanks y = df [ _____ ] . values X = df . drop ( _____ , axis = 1 )","title":"Exercise 1: Split dataset in target property and features"},{"location":"08_ml_matminer/unit-3-exercies/#exercise-2-train-a-random-forest-model-on-the-dataset","text":"Train a random forest model with 150 estimators on the dataset. Next, use the model to get predictions for all samples and store them to the y_pred variable. from sklearn.ensemble import RandomForestRegressor # Fill in the blanks below rf = RandomForestRegressor ( ____ ) rf . fit ( __ , __ ) y_pred = rf . predict ( ___ ) To see how well your model is performing, run the next cell. import numpy as np from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y , y_pred ) print ( 'training RMSE = {:.3f} ' . format ( np . sqrt ( mse )))","title":"Exercise 2: Train a random forest model on the dataset"},{"location":"08_ml_matminer/unit-3-exercies/#exercise-3-evaluate-the-model-using-cross-validation","text":"Evaluate your random forest model using cross validation with 5 splits. This will give a more realistic idea of how well your model will perform in practice. from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score # Fill in the blanks below kfold = KFold ( ___ ) scores = cross_val_score ( __ , __ , __ , scoring = 'neg_mean_squared_error' , cv = ___ ) The final cross validation score can be printed by running the cell below. rmse_scores = [ np . sqrt ( abs ( s )) for s in scores ] print ( 'Mean RMSE: {:.3f} ' . format ( np . mean ( rmse_scores )))","title":"Exercise 3: Evaluate the model using cross validation"},{"location":"08_ml_matminer/unit-3-notes-blank/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Part 3: Machine learning models \u00b6 In parts 1 and 2, we demonstrated how to download a dataset and add machine learnable features. In part 3, we show how to train a machine learning model on a dataset and analyze the results. Scikit-Learn \u00b6 This unit makes extensive use of the scikit-learn package, an open-source python package for machine learning. Matminer has been designed to make machine learning with scikit-learn as easy as possible. Other machine learning packages exist, such as TensorFlow , which implement neural network architectures. These packages can also be used with matminer but are outside the scope of this workshop. Load and prepare a pre-featurized model \u00b6 First, let's load a dataset that we can use for machine learning. In advance, we've added some composition and structure features to the elastic_tensor_2015 dataset used in exercises 1 and 2. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"elastic_tensor_2015_featurized.json\" )) df . head () We first need to split the dataset into the \"target\" property, and the \"features\" used for learning. In this model, we will be using the bulk modulus ( K_VRH ) as the target property. We use the values attribute of the dataframe to give the target properties a numpy array, rather than pandas Series object. The machine learning algorithm can only use numerical features for training. Accordingly, we need to remove any non-numerical columns from our dataset. Additionally, we want to remove the K_VRH column from the set of features, as the model should not know about the target property in advance. The dataset loaded above, includes structure , formula , and composition columns that were previously used to generate the machine learnable features. Let's remove them using the pandas drop() function, discussed in unit 1. Remember, axis=1 indicates we are dropping columns rather than rows. We can see all the descriptors in model using the column attribute. Try a random forest model using scikit-learn \u00b6 The scikit-learn library makes it easy to use our generated features for training machine learning models. It implements a variety of different regression models and contains tools for cross-validation. In the interests of time, in this example we will only trial a single model but it is good practice to trial multiple models to see which performs best for your machine learning problem. A good \"starting\" model is the random forest model. Let's create a random forest model. from sklearn.ensemble import RandomForestRegressor Notice we created the model with the number of estimators ( n_estimators ) set to 100 . n_estimators is an example of a machine learning hyper-parameter . Most models contain many tunable hyper-parameters. To obtain good performance, it is necessary to fine tune these parameters for each individual machine learning problem. There is currently no simple way to know in advance what hyper-parameters will be optimal. Usually, a trial and error approach is used. We can now train our model to use the input features ( X ) to predict the target property ( y ). This is achieved using the fit() function. That's it, we have trained our first machine learning model! Evaluating model performance \u00b6 Next, we need to assess how the model is performing. To do this, we first ask the model to predict the bulk modulus for every entry in our original dataframe. Next, we can check the accuracy of our model by looking at the root mean squared error of our predictions. Scikit-learn provides a mean_squared_error() function to calculate the mean squared error. We then take the square-root of this to obtain our final performance metric. import numpy as np from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y , y_pred ) print ( 'training RMSE = {:.3f} GPa' . format ( np . sqrt ( mse ))) An RMSE of 7.2 GPa looks very reasonable! However, as the model was trained and evaluated on exactly the same data, this is not a true estimate of how the model will perform for unseen materials (the primary purpose of machine learning studies). Cross validation \u00b6 To obtain a more accurate estimate of prediction performance and validate that we are not over-fitting, we need to check the cross-validation score rather than the fitting score. In cross-validation, the data is partitioned randomly into $n$ \"splits\" (in this case 10), each containing roughly the same number of samples. The model is trained on $n-1$ splits (the training set) and the model performance evaluated by comparing the actual and predicted values for the final split (the testing set). In total, this process is repeated $n$ times, such that each split is at some point used as the testing set. The cross-validation score is the average score across all testing sets. There are a number of ways to partition the data into splits. In this example, we use the KFold method and select the number of splits to be 10. I.e., 90 % of the data will be used as the training set, with 10 % used as the testing set. from sklearn.model_selection import KFold Note, we set random_state=1 to ensure every attendee gets the same answer for their model. Finally, obtaining the cross validation score can be automated using the Scikit-Learn cross_val_score() function. This function requires a machine learning model, the input features, and target property as arguments. Note, we pass the kfold object as the cv argument, to make cross_val_score() use the correct test/train splits. For each split, the model will be trained from scratch, before the performance is evaualated. As we have to train and predict 10 times, cross validation can often take some time to perform. In our case, the model is quite small, so the process only takes about a minute. The final cross validation score is the average across all splits. from sklearn.model_selection import cross_val_score scores = cross_val_score ( rf , X , y , scoring = 'neg_mean_squared_error' , cv = kfold ) rmse_scores = [ np . sqrt ( abs ( s )) for s in scores ] print ( 'Mean RMSE: {:.3f} ' . format ( np . mean ( rmse_scores ))) Notice that our RMSE has almost tripled as now it reflects the true predictive power of the model. However, a root-mean-squared error of ~18 GPa is still not bad! Visualizing model performance \u00b6 We can visualize the predictive performance of our model by plotting the our predictions against the actual value, for each sample in the test set for all test/train splits. First, we get the predicted values of the testing set for each split using the cross_val_predict method. This is similar to the cross_val_score method, except it returns the actual predictions, rather than the model score. from sklearn.model_selection import cross_val_predict For plotting we use PlotlyFig module of matminer, which helps you quickly produce publication ready diagrams. PlotlyFig can produce many different types of plots. Explaining its use in detail is outside the scope of this tutorial but examples of the available plots are provided in the FigRecipes section of the matminer_examples repository . from matminer.figrecipes.plot import PlotlyFig pf = PlotlyFig ( x_title = 'DFT (MP) bulk modulus (GPa)' , y_title = 'Predicted bulk modulus (GPa)' , mode = 'notebook' ) pf . xy ( xy_pairs = [( y , y_pred ), ([ 0 , 400 ], [ 0 , 400 ])], labels = df [ 'formula' ], modes = [ 'markers' , 'lines' ], lines = [{}, { 'color' : 'black' , 'dash' : 'dash' }], showlegends = False ) Not too bad! However, there are definitely some outliers (you can hover over the points with your mouse to see what they are). Model interpretation \u00b6 An important aspect of machine learning is being able to understand why a model is making certain predictions. Random forest models are particularly amenable to interpretation as they possess a feature_importances attribute, which contains the importance of each feature in deciding the final prediction. Let's look at the feature importances of our model. To make sense of this, we need to know which feature each number corresponds to. We can use PlotlyFig to plot the importances of the 5 most important features. importances = rf . feature_importances_ included = X . columns . values indices = np . argsort ( importances )[:: - 1 ] pf = PlotlyFig ( y_title = 'Importance (%)' , title = 'Feature by importances' , mode = 'notebook' ) pf . bar ( x = included [ indices ][ 0 : 5 ], y = importances [ indices ][ 0 : 5 ]) As you can see, the average melting point of the elements and the volume per atom are the most important features in our model. Let's practice! \u00b6 Now, let's practice. You'll try training and evaluating a machine learning model on a pre-trained dataset.","title":"Unit 3 notes blank"},{"location":"08_ml_matminer/unit-3-notes-blank/#part-3-machine-learning-models","text":"In parts 1 and 2, we demonstrated how to download a dataset and add machine learnable features. In part 3, we show how to train a machine learning model on a dataset and analyze the results.","title":"Part 3: Machine learning models"},{"location":"08_ml_matminer/unit-3-notes-blank/#scikit-learn","text":"This unit makes extensive use of the scikit-learn package, an open-source python package for machine learning. Matminer has been designed to make machine learning with scikit-learn as easy as possible. Other machine learning packages exist, such as TensorFlow , which implement neural network architectures. These packages can also be used with matminer but are outside the scope of this workshop.","title":"Scikit-Learn"},{"location":"08_ml_matminer/unit-3-notes-blank/#load-and-prepare-a-pre-featurized-model","text":"First, let's load a dataset that we can use for machine learning. In advance, we've added some composition and structure features to the elastic_tensor_2015 dataset used in exercises 1 and 2. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"elastic_tensor_2015_featurized.json\" )) df . head () We first need to split the dataset into the \"target\" property, and the \"features\" used for learning. In this model, we will be using the bulk modulus ( K_VRH ) as the target property. We use the values attribute of the dataframe to give the target properties a numpy array, rather than pandas Series object. The machine learning algorithm can only use numerical features for training. Accordingly, we need to remove any non-numerical columns from our dataset. Additionally, we want to remove the K_VRH column from the set of features, as the model should not know about the target property in advance. The dataset loaded above, includes structure , formula , and composition columns that were previously used to generate the machine learnable features. Let's remove them using the pandas drop() function, discussed in unit 1. Remember, axis=1 indicates we are dropping columns rather than rows. We can see all the descriptors in model using the column attribute.","title":"Load and prepare a pre-featurized model"},{"location":"08_ml_matminer/unit-3-notes-blank/#try-a-random-forest-model-using-scikit-learn","text":"The scikit-learn library makes it easy to use our generated features for training machine learning models. It implements a variety of different regression models and contains tools for cross-validation. In the interests of time, in this example we will only trial a single model but it is good practice to trial multiple models to see which performs best for your machine learning problem. A good \"starting\" model is the random forest model. Let's create a random forest model. from sklearn.ensemble import RandomForestRegressor Notice we created the model with the number of estimators ( n_estimators ) set to 100 . n_estimators is an example of a machine learning hyper-parameter . Most models contain many tunable hyper-parameters. To obtain good performance, it is necessary to fine tune these parameters for each individual machine learning problem. There is currently no simple way to know in advance what hyper-parameters will be optimal. Usually, a trial and error approach is used. We can now train our model to use the input features ( X ) to predict the target property ( y ). This is achieved using the fit() function. That's it, we have trained our first machine learning model!","title":"Try a random forest model using scikit-learn"},{"location":"08_ml_matminer/unit-3-notes-blank/#evaluating-model-performance","text":"Next, we need to assess how the model is performing. To do this, we first ask the model to predict the bulk modulus for every entry in our original dataframe. Next, we can check the accuracy of our model by looking at the root mean squared error of our predictions. Scikit-learn provides a mean_squared_error() function to calculate the mean squared error. We then take the square-root of this to obtain our final performance metric. import numpy as np from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y , y_pred ) print ( 'training RMSE = {:.3f} GPa' . format ( np . sqrt ( mse ))) An RMSE of 7.2 GPa looks very reasonable! However, as the model was trained and evaluated on exactly the same data, this is not a true estimate of how the model will perform for unseen materials (the primary purpose of machine learning studies).","title":"Evaluating model performance"},{"location":"08_ml_matminer/unit-3-notes-blank/#cross-validation","text":"To obtain a more accurate estimate of prediction performance and validate that we are not over-fitting, we need to check the cross-validation score rather than the fitting score. In cross-validation, the data is partitioned randomly into $n$ \"splits\" (in this case 10), each containing roughly the same number of samples. The model is trained on $n-1$ splits (the training set) and the model performance evaluated by comparing the actual and predicted values for the final split (the testing set). In total, this process is repeated $n$ times, such that each split is at some point used as the testing set. The cross-validation score is the average score across all testing sets. There are a number of ways to partition the data into splits. In this example, we use the KFold method and select the number of splits to be 10. I.e., 90 % of the data will be used as the training set, with 10 % used as the testing set. from sklearn.model_selection import KFold Note, we set random_state=1 to ensure every attendee gets the same answer for their model. Finally, obtaining the cross validation score can be automated using the Scikit-Learn cross_val_score() function. This function requires a machine learning model, the input features, and target property as arguments. Note, we pass the kfold object as the cv argument, to make cross_val_score() use the correct test/train splits. For each split, the model will be trained from scratch, before the performance is evaualated. As we have to train and predict 10 times, cross validation can often take some time to perform. In our case, the model is quite small, so the process only takes about a minute. The final cross validation score is the average across all splits. from sklearn.model_selection import cross_val_score scores = cross_val_score ( rf , X , y , scoring = 'neg_mean_squared_error' , cv = kfold ) rmse_scores = [ np . sqrt ( abs ( s )) for s in scores ] print ( 'Mean RMSE: {:.3f} ' . format ( np . mean ( rmse_scores ))) Notice that our RMSE has almost tripled as now it reflects the true predictive power of the model. However, a root-mean-squared error of ~18 GPa is still not bad!","title":"Cross validation"},{"location":"08_ml_matminer/unit-3-notes-blank/#visualizing-model-performance","text":"We can visualize the predictive performance of our model by plotting the our predictions against the actual value, for each sample in the test set for all test/train splits. First, we get the predicted values of the testing set for each split using the cross_val_predict method. This is similar to the cross_val_score method, except it returns the actual predictions, rather than the model score. from sklearn.model_selection import cross_val_predict For plotting we use PlotlyFig module of matminer, which helps you quickly produce publication ready diagrams. PlotlyFig can produce many different types of plots. Explaining its use in detail is outside the scope of this tutorial but examples of the available plots are provided in the FigRecipes section of the matminer_examples repository . from matminer.figrecipes.plot import PlotlyFig pf = PlotlyFig ( x_title = 'DFT (MP) bulk modulus (GPa)' , y_title = 'Predicted bulk modulus (GPa)' , mode = 'notebook' ) pf . xy ( xy_pairs = [( y , y_pred ), ([ 0 , 400 ], [ 0 , 400 ])], labels = df [ 'formula' ], modes = [ 'markers' , 'lines' ], lines = [{}, { 'color' : 'black' , 'dash' : 'dash' }], showlegends = False ) Not too bad! However, there are definitely some outliers (you can hover over the points with your mouse to see what they are).","title":"Visualizing model performance"},{"location":"08_ml_matminer/unit-3-notes-blank/#model-interpretation","text":"An important aspect of machine learning is being able to understand why a model is making certain predictions. Random forest models are particularly amenable to interpretation as they possess a feature_importances attribute, which contains the importance of each feature in deciding the final prediction. Let's look at the feature importances of our model. To make sense of this, we need to know which feature each number corresponds to. We can use PlotlyFig to plot the importances of the 5 most important features. importances = rf . feature_importances_ included = X . columns . values indices = np . argsort ( importances )[:: - 1 ] pf = PlotlyFig ( y_title = 'Importance (%)' , title = 'Feature by importances' , mode = 'notebook' ) pf . bar ( x = included [ indices ][ 0 : 5 ], y = importances [ indices ][ 0 : 5 ]) As you can see, the average melting point of the elements and the volume per atom are the most important features in our model.","title":"Model interpretation"},{"location":"08_ml_matminer/unit-3-notes-blank/#lets-practice","text":"Now, let's practice. You'll try training and evaluating a machine learning model on a pre-trained dataset.","title":"Let's practice!"},{"location":"08_ml_matminer/unit-3-notes/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Part 3: Machine learning models \u00b6 In parts 1 and 2, we demonstrated how to download a dataset and add machine learnable features. In part 3, we show how to train a machine learning model on a dataset and analyze the results. Scikit-Learn \u00b6 This unit makes extensive use of the scikit-learn package, an open-source python package for machine learning. Matminer has been designed to make machine learning with scikit-learn as easy as possible. Other machine learning packages exist, such as TensorFlow , which implement neural network architectures. These packages can also be used with matminer but are outside the scope of this workshop. Load and prepare a pre-featurized model \u00b6 First, let's load a dataset that we can use for machine learning. In advance, we've added some composition and structure features to the elastic_tensor_2015 dataset used in exercises 1 and 2. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"elastic_tensor_2015_featurized.json\" )) df . head () We first need to split the dataset into the \"target\" property, and the \"features\" used for learning. In this model, we will be using the bulk modulus ( K_VRH ) as the target property. We use the values attribute of the dataframe to give the target properties a numpy array, rather than pandas Series object. y = df [ 'K_VRH' ] . values print ( y ) The machine learning algorithm can only use numerical features for training. Accordingly, we need to remove any non-numerical columns from our dataset. Additionally, we want to remove the K_VRH column from the set of features, as the model should not know about the target property in advance. The dataset loaded above, includes structure , formula , and composition columns that were previously used to generate the machine learnable features. Let's remove them using the pandas drop() function, discussed in unit 1. Remember, axis=1 indicates we are dropping columns rather than rows. X = df . drop ([ \"structure\" , \"formula\" , \"composition\" , \"K_VRH\" ], axis = 1 ) We can see all the descriptors in model using the column attribute. print ( \"There are {} possible descriptors:\" . format ( X . columns )) print ( X . columns ) Try a random forest model using scikit-learn \u00b6 The scikit-learn library makes it easy to use our generated features for training machine learning models. It implements a variety of different regression models and contains tools for cross-validation. In the interests of time, in this example we will only trial a single model but it is good practice to trial multiple models to see which performs best for your machine learning problem. A good \"starting\" model is the random forest model. Let's create a random forest model. from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor ( n_estimators = 100 , random_state = 1 ) Notice we created the model with the number of estimators ( n_estimators ) set to 100 . n_estimators is an example of a machine learning hyper-parameter . Most models contain many tunable hyper-parameters. To obtain good performance, it is necessary to fine tune these parameters for each individual machine learning problem. There is currently no simple way to know in advance what hyper-parameters will be optimal. Usually, a trial and error approach is used. We can now train our model to use the input features ( X ) to predict the target property ( y ). This is achieved using the fit() function. rf . fit ( X , y ) That's it, we have trained our first machine learning model! Evaluating model performance \u00b6 Next, we need to assess how the model is performing. To do this, we first ask the model to predict the bulk modulus for every entry in our original dataframe. y_pred = rf . predict ( X ) Next, we can check the accuracy of our model by looking at the root mean squared error of our predictions. Scikit-learn provides a mean_squared_error() function to calculate the mean squared error. We then take the square-root of this to obtain our final performance metric. import numpy as np from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y , y_pred ) print ( 'training RMSE = {:.3f} GPa' . format ( np . sqrt ( mse ))) An RMSE of 7.2 GPa looks very reasonable! However, as the model was trained and evaluated on exactly the same data, this is not a true estimate of how the model will perform for unseen materials (the primary purpose of machine learning studies). Cross validation \u00b6 To obtain a more accurate estimate of prediction performance and validate that we are not over-fitting, we need to check the cross-validation score rather than the fitting score. In cross-validation, the data is partitioned randomly into $n$ \"splits\" (in this case 10), each containing roughly the same number of samples. The model is trained on $n-1$ splits (the training set) and the model performance evaluated by comparing the actual and predicted values for the final split (the testing set). In total, this process is repeated $n$ times, such that each split is at some point used as the testing set. The cross-validation score is the average score across all testing sets. There are a number of ways to partition the data into splits. In this example, we use the KFold method and select the number of splits to be 10. I.e., 90 % of the data will be used as the training set, with 10 % used as the testing set. from sklearn.model_selection import KFold kfold = KFold ( n_splits = 10 , random_state = 1 ) Note, we set random_state=1 to ensure every attendee gets the same answer for their model. Finally, obtaining the cross validation score can be automated using the Scikit-Learn cross_val_score() function. This function requires a machine learning model, the input features, and target property as arguments. Note, we pass the kfold object as the cv argument, to make cross_val_score() use the correct test/train splits. For each split, the model will be trained from scratch, before the performance is evaualated. As we have to train and predict 10 times, cross validation can often take some time to perform. In our case, the model is quite small, so the process only takes about a minute. The final cross validation score is the average across all splits. from sklearn.model_selection import cross_val_score scores = cross_val_score ( rf , X , y , scoring = 'neg_mean_squared_error' , cv = kfold ) rmse_scores = [ np . sqrt ( abs ( s )) for s in scores ] print ( 'Mean RMSE: {:.3f} ' . format ( np . mean ( rmse_scores ))) Notice that our RMSE has almost tripled as now it reflects the true predictive power of the model. However, a root-mean-squared error of ~18 GPa is still not bad! Visualizing model performance \u00b6 We can visualize the predictive performance of our model by plotting the our predictions against the actual value, for each sample in the test set for all test/train splits. First, we get the predicted values of the testing set for each split using the cross_val_predict method. This is similar to the cross_val_score method, except it returns the actual predictions, rather than the model score. from sklearn.model_selection import cross_val_predict y_pred = cross_val_predict ( rf , X , y , cv = kfold ) For plotting we use PlotlyFig module of matminer, which helps you quickly produce publication ready diagrams. PlotlyFig can produce many different types of plots. Explaining its use in detail is outside the scope of this tutorial but examples of the available plots are provided in the FigRecipes section of the matminer_examples repository . from matminer.figrecipes.plot import PlotlyFig pf = PlotlyFig ( x_title = 'DFT (MP) bulk modulus (GPa)' , y_title = 'Predicted bulk modulus (GPa)' , mode = 'notebook' ) pf . xy ( xy_pairs = [( y , y_pred ), ([ 0 , 400 ], [ 0 , 400 ])], labels = df [ 'formula' ], modes = [ 'markers' , 'lines' ], lines = [{}, { 'color' : 'black' , 'dash' : 'dash' }], showlegends = False ) Not too bad! However, there are definitely some outliers (you can hover over the points with your mouse to see what they are). Model interpretation \u00b6 An important aspect of machine learning is being able to understand why a model is making certain predictions. Random forest models are particularly amenable to interpretation as they possess a feature_importances attribute, which contains the importance of each feature in deciding the final prediction. Let's look at the feature importances of our model. rf . feature_importances_ To make sense of this, we need to know which feature each number corresponds to. We can use PlotlyFig to plot the importances of the 5 most important features. importances = rf . feature_importances_ included = X . columns . values indices = np . argsort ( importances )[:: - 1 ] pf = PlotlyFig ( y_title = 'Importance (%)' , title = 'Feature by importances' , mode = 'notebook' ) pf . bar ( x = included [ indices ][ 0 : 5 ], y = importances [ indices ][ 0 : 5 ]) As you can see, the average melting point of the elements and the volume per atom are the most important features in our model. Let's practice! \u00b6 Now, let's practice. You'll try training and evaluating a machine learning model on a pre-trained dataset.","title":"Unit 3 notes"},{"location":"08_ml_matminer/unit-3-notes/#part-3-machine-learning-models","text":"In parts 1 and 2, we demonstrated how to download a dataset and add machine learnable features. In part 3, we show how to train a machine learning model on a dataset and analyze the results.","title":"Part 3: Machine learning models"},{"location":"08_ml_matminer/unit-3-notes/#scikit-learn","text":"This unit makes extensive use of the scikit-learn package, an open-source python package for machine learning. Matminer has been designed to make machine learning with scikit-learn as easy as possible. Other machine learning packages exist, such as TensorFlow , which implement neural network architectures. These packages can also be used with matminer but are outside the scope of this workshop.","title":"Scikit-Learn"},{"location":"08_ml_matminer/unit-3-notes/#load-and-prepare-a-pre-featurized-model","text":"First, let's load a dataset that we can use for machine learning. In advance, we've added some composition and structure features to the elastic_tensor_2015 dataset used in exercises 1 and 2. import os from matminer.utils.io import load_dataframe_from_json df = load_dataframe_from_json ( os . path . join ( \"resources\" , \"elastic_tensor_2015_featurized.json\" )) df . head () We first need to split the dataset into the \"target\" property, and the \"features\" used for learning. In this model, we will be using the bulk modulus ( K_VRH ) as the target property. We use the values attribute of the dataframe to give the target properties a numpy array, rather than pandas Series object. y = df [ 'K_VRH' ] . values print ( y ) The machine learning algorithm can only use numerical features for training. Accordingly, we need to remove any non-numerical columns from our dataset. Additionally, we want to remove the K_VRH column from the set of features, as the model should not know about the target property in advance. The dataset loaded above, includes structure , formula , and composition columns that were previously used to generate the machine learnable features. Let's remove them using the pandas drop() function, discussed in unit 1. Remember, axis=1 indicates we are dropping columns rather than rows. X = df . drop ([ \"structure\" , \"formula\" , \"composition\" , \"K_VRH\" ], axis = 1 ) We can see all the descriptors in model using the column attribute. print ( \"There are {} possible descriptors:\" . format ( X . columns )) print ( X . columns )","title":"Load and prepare a pre-featurized model"},{"location":"08_ml_matminer/unit-3-notes/#try-a-random-forest-model-using-scikit-learn","text":"The scikit-learn library makes it easy to use our generated features for training machine learning models. It implements a variety of different regression models and contains tools for cross-validation. In the interests of time, in this example we will only trial a single model but it is good practice to trial multiple models to see which performs best for your machine learning problem. A good \"starting\" model is the random forest model. Let's create a random forest model. from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor ( n_estimators = 100 , random_state = 1 ) Notice we created the model with the number of estimators ( n_estimators ) set to 100 . n_estimators is an example of a machine learning hyper-parameter . Most models contain many tunable hyper-parameters. To obtain good performance, it is necessary to fine tune these parameters for each individual machine learning problem. There is currently no simple way to know in advance what hyper-parameters will be optimal. Usually, a trial and error approach is used. We can now train our model to use the input features ( X ) to predict the target property ( y ). This is achieved using the fit() function. rf . fit ( X , y ) That's it, we have trained our first machine learning model!","title":"Try a random forest model using scikit-learn"},{"location":"08_ml_matminer/unit-3-notes/#evaluating-model-performance","text":"Next, we need to assess how the model is performing. To do this, we first ask the model to predict the bulk modulus for every entry in our original dataframe. y_pred = rf . predict ( X ) Next, we can check the accuracy of our model by looking at the root mean squared error of our predictions. Scikit-learn provides a mean_squared_error() function to calculate the mean squared error. We then take the square-root of this to obtain our final performance metric. import numpy as np from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y , y_pred ) print ( 'training RMSE = {:.3f} GPa' . format ( np . sqrt ( mse ))) An RMSE of 7.2 GPa looks very reasonable! However, as the model was trained and evaluated on exactly the same data, this is not a true estimate of how the model will perform for unseen materials (the primary purpose of machine learning studies).","title":"Evaluating model performance"},{"location":"08_ml_matminer/unit-3-notes/#cross-validation","text":"To obtain a more accurate estimate of prediction performance and validate that we are not over-fitting, we need to check the cross-validation score rather than the fitting score. In cross-validation, the data is partitioned randomly into $n$ \"splits\" (in this case 10), each containing roughly the same number of samples. The model is trained on $n-1$ splits (the training set) and the model performance evaluated by comparing the actual and predicted values for the final split (the testing set). In total, this process is repeated $n$ times, such that each split is at some point used as the testing set. The cross-validation score is the average score across all testing sets. There are a number of ways to partition the data into splits. In this example, we use the KFold method and select the number of splits to be 10. I.e., 90 % of the data will be used as the training set, with 10 % used as the testing set. from sklearn.model_selection import KFold kfold = KFold ( n_splits = 10 , random_state = 1 ) Note, we set random_state=1 to ensure every attendee gets the same answer for their model. Finally, obtaining the cross validation score can be automated using the Scikit-Learn cross_val_score() function. This function requires a machine learning model, the input features, and target property as arguments. Note, we pass the kfold object as the cv argument, to make cross_val_score() use the correct test/train splits. For each split, the model will be trained from scratch, before the performance is evaualated. As we have to train and predict 10 times, cross validation can often take some time to perform. In our case, the model is quite small, so the process only takes about a minute. The final cross validation score is the average across all splits. from sklearn.model_selection import cross_val_score scores = cross_val_score ( rf , X , y , scoring = 'neg_mean_squared_error' , cv = kfold ) rmse_scores = [ np . sqrt ( abs ( s )) for s in scores ] print ( 'Mean RMSE: {:.3f} ' . format ( np . mean ( rmse_scores ))) Notice that our RMSE has almost tripled as now it reflects the true predictive power of the model. However, a root-mean-squared error of ~18 GPa is still not bad!","title":"Cross validation"},{"location":"08_ml_matminer/unit-3-notes/#visualizing-model-performance","text":"We can visualize the predictive performance of our model by plotting the our predictions against the actual value, for each sample in the test set for all test/train splits. First, we get the predicted values of the testing set for each split using the cross_val_predict method. This is similar to the cross_val_score method, except it returns the actual predictions, rather than the model score. from sklearn.model_selection import cross_val_predict y_pred = cross_val_predict ( rf , X , y , cv = kfold ) For plotting we use PlotlyFig module of matminer, which helps you quickly produce publication ready diagrams. PlotlyFig can produce many different types of plots. Explaining its use in detail is outside the scope of this tutorial but examples of the available plots are provided in the FigRecipes section of the matminer_examples repository . from matminer.figrecipes.plot import PlotlyFig pf = PlotlyFig ( x_title = 'DFT (MP) bulk modulus (GPa)' , y_title = 'Predicted bulk modulus (GPa)' , mode = 'notebook' ) pf . xy ( xy_pairs = [( y , y_pred ), ([ 0 , 400 ], [ 0 , 400 ])], labels = df [ 'formula' ], modes = [ 'markers' , 'lines' ], lines = [{}, { 'color' : 'black' , 'dash' : 'dash' }], showlegends = False ) Not too bad! However, there are definitely some outliers (you can hover over the points with your mouse to see what they are).","title":"Visualizing model performance"},{"location":"08_ml_matminer/unit-3-notes/#model-interpretation","text":"An important aspect of machine learning is being able to understand why a model is making certain predictions. Random forest models are particularly amenable to interpretation as they possess a feature_importances attribute, which contains the importance of each feature in deciding the final prediction. Let's look at the feature importances of our model. rf . feature_importances_ To make sense of this, we need to know which feature each number corresponds to. We can use PlotlyFig to plot the importances of the 5 most important features. importances = rf . feature_importances_ included = X . columns . values indices = np . argsort ( importances )[:: - 1 ] pf = PlotlyFig ( y_title = 'Importance (%)' , title = 'Feature by importances' , mode = 'notebook' ) pf . bar ( x = included [ indices ][ 0 : 5 ], y = importances [ indices ][ 0 : 5 ]) As you can see, the average melting point of the elements and the volume per atom are the most important features in our model.","title":"Model interpretation"},{"location":"08_ml_matminer/unit-3-notes/#lets-practice","text":"Now, let's practice. You'll try training and evaluating a machine learning model on a pre-trained dataset.","title":"Let's practice!"},{"location":"primer/01_basic_python/1%20-%20Introduction%20and%20Jupyter%20Use/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 1: Introduction to Python and Using Jupyter notebooks \u00b6 The Jupyter notebook is a platform for using and writing code in a dynamic way that allows users to combine cells of code snippets that are executed with a persistent namespace and kernel alongside markdown text for facilitating readability and visualization. In this notebook, we cover some basics of Jupyter functionality along with a discussion of some details of how you can use python in this environment and elsewhere. Installation \u00b6 For this workshop, we've constructed an online environment for everyone to use in order to smooth out any platform-dependent installation issues, but you'll probably want to install the tools we use today locally on your own machine. To do this, we recommend Anaconda , which is an effective tool for python package management that can create virtual environments, comes with a pre-installed IDE, and includes all of the Jupyter functionality that you'll see here. The Anaconda installer should be detailed on the page linked above, but here's another resource for installation that might be helpful. Code vs. Markdown \u00b6 Jupyter notebooks are broken down into \"cells\" which might contain either code or markdown. If you select a cell with your mouse, it should be highlighted with a green border indicating that you are in \"edit,\" mode and can edit the contents of the cell. If text reading In [ ]: is on the left hand side of the cell, it's a \"code\" cell. For example, type the following and press \"Shift + Enter\" print ( \"Hello world!\" ) Pressing \"Shift + Enter\" executes the code in the cell, prints the output below the cell, and creates a new cell below that one. In addition to code cells, you can also write your own markdown cells by converting a cell using either the dropdown menu in the toolbar or pressing \"Esc + m\". In general, pressing escape enters \"command mode\" for which you can issue a number of commands, including f - find and replace m - convert to markdown y - convert to code h - open the help menu Right now, trying typing \"Esc + h\" to open the help/shortcut menu and peruse it. Spend a minute testing out some of the shortcuts. Note that markdown cells are quite flexible and can basically do anything wikipedia does, including adding $\\LaTeX$-formatted equations. $\\hat{H}\\psi = E \\psi$ Shell commands, magic, and where to learn more \u00b6 Jupyter notebooks can also issue commands to the shell, which can be achieved using the ! symbol at the beginning of the cell: ! ls . ! date In addition, certain things can be achieved in Jupyter notebooks using what are called \"magic\" commands, which are demarcated using the % sign. The most common of these are the magic function to enable inline plotting: % matplotlib inline and to invoke the debugger in a particular cell on an error: % pdb These functions set up special functionality in the notebook. Lastly, note that Jupyter notebooks are becoming increasingly popular as tools to supplement publication. As a computational researcher, you can provide explicit documentation of your methods with embedded code that actually works for a person who wants to understand better what you're working on. In my own work, I've begun making all my plots and collecting all of my data in Jupyter notebooks to provide as supporting info for each of my recent papers. It's a bit more work, but you'll find that having this level of organization and being this transparent about your methods goes a long way. There are great resources for IPython notebooks online, and here are a few of them: Jupyter website A gallery of interesting notebooks","title":"1   Introduction and Jupyter Use"},{"location":"primer/01_basic_python/1%20-%20Introduction%20and%20Jupyter%20Use/#lesson-1-introduction-to-python-and-using-jupyter-notebooks","text":"The Jupyter notebook is a platform for using and writing code in a dynamic way that allows users to combine cells of code snippets that are executed with a persistent namespace and kernel alongside markdown text for facilitating readability and visualization. In this notebook, we cover some basics of Jupyter functionality along with a discussion of some details of how you can use python in this environment and elsewhere.","title":"Lesson 1: Introduction to Python and Using Jupyter notebooks"},{"location":"primer/01_basic_python/1%20-%20Introduction%20and%20Jupyter%20Use/#installation","text":"For this workshop, we've constructed an online environment for everyone to use in order to smooth out any platform-dependent installation issues, but you'll probably want to install the tools we use today locally on your own machine. To do this, we recommend Anaconda , which is an effective tool for python package management that can create virtual environments, comes with a pre-installed IDE, and includes all of the Jupyter functionality that you'll see here. The Anaconda installer should be detailed on the page linked above, but here's another resource for installation that might be helpful.","title":"Installation"},{"location":"primer/01_basic_python/1%20-%20Introduction%20and%20Jupyter%20Use/#code-vs-markdown","text":"Jupyter notebooks are broken down into \"cells\" which might contain either code or markdown. If you select a cell with your mouse, it should be highlighted with a green border indicating that you are in \"edit,\" mode and can edit the contents of the cell. If text reading In [ ]: is on the left hand side of the cell, it's a \"code\" cell. For example, type the following and press \"Shift + Enter\" print ( \"Hello world!\" ) Pressing \"Shift + Enter\" executes the code in the cell, prints the output below the cell, and creates a new cell below that one. In addition to code cells, you can also write your own markdown cells by converting a cell using either the dropdown menu in the toolbar or pressing \"Esc + m\". In general, pressing escape enters \"command mode\" for which you can issue a number of commands, including f - find and replace m - convert to markdown y - convert to code h - open the help menu Right now, trying typing \"Esc + h\" to open the help/shortcut menu and peruse it. Spend a minute testing out some of the shortcuts. Note that markdown cells are quite flexible and can basically do anything wikipedia does, including adding $\\LaTeX$-formatted equations. $\\hat{H}\\psi = E \\psi$","title":"Code vs. Markdown"},{"location":"primer/01_basic_python/1%20-%20Introduction%20and%20Jupyter%20Use/#shell-commands-magic-and-where-to-learn-more","text":"Jupyter notebooks can also issue commands to the shell, which can be achieved using the ! symbol at the beginning of the cell: ! ls . ! date In addition, certain things can be achieved in Jupyter notebooks using what are called \"magic\" commands, which are demarcated using the % sign. The most common of these are the magic function to enable inline plotting: % matplotlib inline and to invoke the debugger in a particular cell on an error: % pdb These functions set up special functionality in the notebook. Lastly, note that Jupyter notebooks are becoming increasingly popular as tools to supplement publication. As a computational researcher, you can provide explicit documentation of your methods with embedded code that actually works for a person who wants to understand better what you're working on. In my own work, I've begun making all my plots and collecting all of my data in Jupyter notebooks to provide as supporting info for each of my recent papers. It's a bit more work, but you'll find that having this level of organization and being this transparent about your methods goes a long way. There are great resources for IPython notebooks online, and here are a few of them: Jupyter website A gallery of interesting notebooks","title":"Shell commands, magic, and where to learn more"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 2: Expressions, variables and built-in functions \u00b6 Expressions \u00b6 An expression describes a computation and evaluates to a value. 2019 2019 2000 + 19 2019 1 * 2 * ( 3 * 4 ) 24 A call expression is when you name the function you want to call and write down the expression you want the function to apply to. max ( 1 , 4 ) 4 min ( - 1 , 2019 ) -1 min ( max ( 1 , 3 ), 4 ) 3 min ( 1 , 2 , 3 , 4 ) 1 Variables \u00b6 * Variables are names for values. * In Python the = symbol assigns the value on the right to the name on the left. * The variable is created when a value is assigned to it. Here, Python assigns an age to a variable age and a name in quotation marks to a variable first_name. age = 27 first_name = \"Tingzheng\" last_name = \"Hou\" Unlike many other languages, Python uses a convention known as \"duck typing,\" meaning it infers the data type of a given variable from the context. Common data types include int, float, string, NoneType. You can find the type of a variable using the type function. type ( age ) int To see duck typing in action, try defining age as 42.0 and inspecting the type: age = 42.0 type ( age ) float In the Jupyter notebook, variable names are preserved between cells. In more formal language, the namespace of a notebook is preserved unless the kernel is reset, which resets all of the variable names. You can see this if you execute the following cells out of sequence. Try it out. Also try resetting the kernel and executing just the latter half. age = 42 age 42 age = 29 age 29 Built-in functions \u00b6 Now that we understand how python and jupyter handle variables, let's talk about built-in functions. We've actually already seen min , max , type . Let's try another built-in function (or one that's available by default without issuing any special instructions) called print . print ( \"Hello world!\" ) Hello world! Functions are called using the function name and parentheses, which enclose the function arguments . In the previous example, print is the function, and the string \"Hello world!\" is the argument. Let's experiment with print a little: print ( \"Age is\" , age ) Age is 29 print ( type ( last_name )) <class 'str'> One of the most important things you can know when learning programming is where to access help on the things you're trying to use. There are a number of ways to access information on a function. Typically, I like to use \"Shift+Tab,\" which brings up a small box with function information, which can be expanded by pressing tab again. help ( print ) print ? Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream. Exercise: What is the usage of bin ? \u00b6 bin ( 10 ) '0b1010' Exercise (Optional): What will you see with the print(print(10)) ? Why? \u00b6 print ( print ( 10 )) 10 None Importing other modules and functions \u00b6 Here's a list of built-in functions , but we often need functionality that's not included in this set. To get a function that's not included in our namespace by default, we use the import statement. In this example, we're going to import a module called math that contains a number of functions that can be accessed by using math.FUNCTION_NAME_HERE . import math print ( \"Exponent of 2:\" , math . exp ( 2 )) print ( \"Sine of 3.1415:\" , math . sin ( 3.1415 )) Exponent of 2: 7.38905609893065 Sine of 3.1415: 9.265358966049024e-05 Note that we can also import functions from modules like so: from operator import add , mul from math import log , factorial add ( 1 , 2018 ) 2019 print ( factorial ( 4 )) 24 Venturing outside the standard library \u00b6 The Standard library includes a number of modules that come pre-installed with the python programming language itself, which is summarized in this documentation . Python's real power, however, comes in how extensible it is. Python has an ecosystem of external libraries that can be managed using tools like the aforementioned Anaconda and pip. We've installed everything you need for this workshop, but if you want to install an external library on your own machine, you can issue a command like conda install pandas at the command line to automatically install and link a compatable version of the pandas dataframe management library to your current environment. Three of the most common external libraries in python are: numpy - for efficient numerical array manipulation and operation scipy - for a number of tools related to scientific computing matplotlib - for plotting data and creating figures import numpy as np from matplotlib import pyplot inflammation = np . loadtxt ( \"../../mp_workshop/data_files/inflammation-04.csv\" , delimiter = ',' ) % matplotlib inline average = np . average ( inflammation , axis = 0 ) pyplot . plot ( average ) [<matplotlib.lines.Line2D at 0x1113f1438>] You can even observe crystal structures in line.","title":"2   Variables and built in functions"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/#lesson-2-expressions-variables-and-built-in-functions","text":"","title":"Lesson 2: Expressions, variables and built-in functions"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/#expressions","text":"An expression describes a computation and evaluates to a value. 2019 2019 2000 + 19 2019 1 * 2 * ( 3 * 4 ) 24 A call expression is when you name the function you want to call and write down the expression you want the function to apply to. max ( 1 , 4 ) 4 min ( - 1 , 2019 ) -1 min ( max ( 1 , 3 ), 4 ) 3 min ( 1 , 2 , 3 , 4 ) 1","title":"Expressions"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/#variables","text":"* Variables are names for values. * In Python the = symbol assigns the value on the right to the name on the left. * The variable is created when a value is assigned to it. Here, Python assigns an age to a variable age and a name in quotation marks to a variable first_name. age = 27 first_name = \"Tingzheng\" last_name = \"Hou\" Unlike many other languages, Python uses a convention known as \"duck typing,\" meaning it infers the data type of a given variable from the context. Common data types include int, float, string, NoneType. You can find the type of a variable using the type function. type ( age ) int To see duck typing in action, try defining age as 42.0 and inspecting the type: age = 42.0 type ( age ) float In the Jupyter notebook, variable names are preserved between cells. In more formal language, the namespace of a notebook is preserved unless the kernel is reset, which resets all of the variable names. You can see this if you execute the following cells out of sequence. Try it out. Also try resetting the kernel and executing just the latter half. age = 42 age 42 age = 29 age 29","title":"Variables"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/#built-in-functions","text":"Now that we understand how python and jupyter handle variables, let's talk about built-in functions. We've actually already seen min , max , type . Let's try another built-in function (or one that's available by default without issuing any special instructions) called print . print ( \"Hello world!\" ) Hello world! Functions are called using the function name and parentheses, which enclose the function arguments . In the previous example, print is the function, and the string \"Hello world!\" is the argument. Let's experiment with print a little: print ( \"Age is\" , age ) Age is 29 print ( type ( last_name )) <class 'str'> One of the most important things you can know when learning programming is where to access help on the things you're trying to use. There are a number of ways to access information on a function. Typically, I like to use \"Shift+Tab,\" which brings up a small box with function information, which can be expanded by pressing tab again. help ( print ) print ? Help on built-in function print in module builtins: print(...) print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False) Prints the values to a stream, or to sys.stdout by default. Optional keyword arguments: file: a file-like object (stream); defaults to the current sys.stdout. sep: string inserted between values, default a space. end: string appended after the last value, default a newline. flush: whether to forcibly flush the stream.","title":"Built-in functions"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/#exercise-what-is-the-usage-of-bin","text":"bin ( 10 ) '0b1010'","title":"Exercise: What is the usage of bin?"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/#exercise-optional-what-will-you-see-with-the-printprint10-why","text":"print ( print ( 10 )) 10 None","title":"Exercise (Optional): What will you see with the  print(print(10))? Why?"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/#importing-other-modules-and-functions","text":"Here's a list of built-in functions , but we often need functionality that's not included in this set. To get a function that's not included in our namespace by default, we use the import statement. In this example, we're going to import a module called math that contains a number of functions that can be accessed by using math.FUNCTION_NAME_HERE . import math print ( \"Exponent of 2:\" , math . exp ( 2 )) print ( \"Sine of 3.1415:\" , math . sin ( 3.1415 )) Exponent of 2: 7.38905609893065 Sine of 3.1415: 9.265358966049024e-05 Note that we can also import functions from modules like so: from operator import add , mul from math import log , factorial add ( 1 , 2018 ) 2019 print ( factorial ( 4 )) 24","title":"Importing other modules and functions"},{"location":"primer/01_basic_python/2%20-%20Variables%20and%20built-in%20functions/#venturing-outside-the-standard-library","text":"The Standard library includes a number of modules that come pre-installed with the python programming language itself, which is summarized in this documentation . Python's real power, however, comes in how extensible it is. Python has an ecosystem of external libraries that can be managed using tools like the aforementioned Anaconda and pip. We've installed everything you need for this workshop, but if you want to install an external library on your own machine, you can issue a command like conda install pandas at the command line to automatically install and link a compatable version of the pandas dataframe management library to your current environment. Three of the most common external libraries in python are: numpy - for efficient numerical array manipulation and operation scipy - for a number of tools related to scientific computing matplotlib - for plotting data and creating figures import numpy as np from matplotlib import pyplot inflammation = np . loadtxt ( \"../../mp_workshop/data_files/inflammation-04.csv\" , delimiter = ',' ) % matplotlib inline average = np . average ( inflammation , axis = 0 ) pyplot . plot ( average ) [<matplotlib.lines.Line2D at 0x1113f1438>] You can even observe crystal structures in line.","title":"Venturing outside the standard library"},{"location":"primer/01_basic_python/3%20-%20Lists/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 3: Lists \u00b6 Teaching: 15min Exercises: 5min A list stores many values in a single structure. \u00b6 Doing calculations with a hundred variables called pressure_001 , pressure_002 , etc., would be at least as slow as doing them by hand. Use a list to store many values together. Contained within square brackets [...] . Values separated by commas , . Use len to find out how many values are in a list. pressures = [ 0.273 , 0.275 , 0.277 , 0.275 , 0.276 ] print ( 'pressures:' , pressures ) print ( 'length:' , len ( pressures )) Use an item's index to fetch it from a list \u00b6 just like strings print ( 'zeroth item of pressures:' , pressures [ 0 ]) print ( 'fourth item of pressures:' , pressures [ 4 ]) print ( 'fourth item of \"pressures\":' , \"pressures\" [ 4 ]) Say your name ... Lists' values can be replaced by assigning to them \u00b6 Use an index expression on the left of assignment to replace a value. pressures [ 0 ] = 0.265 print ( 'pressures is now:' , pressures ) pressures is now: [0.265, 0.275, 0.277, 0.275, 0.276] Appending items to a list lengthens it \u00b6 use list_name.append to add items to the end of a list. primes = [ 2 , 3 , 5 ] print ( 'primes is initially:' , primes ) primes . append ( 7 ) primes . append ( 9 ) print ( 'primes has become:' , primes ) primes is initially: [2, 3, 5] primes has become: [2, 3, 5, 7, 9] * append is a method of lists * like a function, but tied to a particular object Use object_name.method_name to call methods deliberately resembles the way we refer to things in a library We will meet other methods of lists as we go along Use help(list) for a preview extend is similar to append , but it allows you to combine two lists. teen_primes = [ 11 , 13 , 17 , 19 ] older_primes = [ 23 , 31 , 47 , 59 ] print ( 'primes is currently:' , primes ) primes . extend ( teen_primes ) print ( 'primes has now become:' , primes ) primes . append ( older_primes ) print ( 'primes has finally become:' , primes ) primes is currently: [2, 3, 5, 7, 9] primes has now become: [2, 3, 5, 7, 9, 11, 13, 17, 19] primes has finally become: [2, 3, 5, 7, 9, 11, 13, 17, 19, [23, 31, 47, 59]] Note that while extend maintains the \"flat\" structure of the list, appending a list to a list makes the result two-dimensional. Use del to remove items from a list entirely \u00b6 del list_name[index] removes an item from a list and shortens the list. Not a function or method, but a statement in the language. print ( 'primes before removing last item:' , primes ) del primes [ 4 ] print ( 'primes after removing last item:' , primes ) primes before removing last item: [2, 3, 5, 7, 9, 11, 13, 17, 19, [23, 31, 47, 59]] primes after removing last item: [2, 3, 5, 7, 11, 13, 17, 19, [23, 31, 47, 59]] The empty list contains no values \u00b6 [] is \"the zero of lists\" Helpful as a starting point for collecting values Lists may be heterogeneous \u00b6 goals = [ 1 , 'Create lists.' , 2 , 'Extract items from lists.' , 3 , 'Modify lists.' ] * Not always a good idea... Character strings are immutable \u00b6 Cannot change the characters in a string after it has been created. Python considers the string to be a single value with parts, not a collection of values. import traceback element = 'helium' try : element [ 0 ] = 'C' except TypeError : print ( traceback . format_exc ()) Traceback (most recent call last): File \"<ipython-input-9-7b43d8c316ad>\", line 5, in <module> element[0] = 'C' TypeError: 'str' object does not support item assignment Index and slice to get information out of a string/list \u00b6 Locations are numbered from 0 rather than 1 Negative indices count backward from the end of the string Slices include the lower bound but exclude the upper bound, so (upper - lower) is the slice's length print ( 'first character:' , element [ 0 ]) print ( 'last character:' , element [ - 1 ]) print ( 'middle:' , element [ 2 : 5 ]) first character: h last character: m middle: liu Indexing beyond the end of a collection is an error try : print ( '99th element of element is:' , element [ 99 ]) except IndexError : print ( traceback . format_exc ()) Traceback (most recent call last): File \"<ipython-input-11-68966f3afd5b>\", line 2, in <module> print('99th element of element is:', element[99]) IndexError: string index out of range Use the built-in function len to find the length of a string \u00b6 print ( len ( 'helium' )) 6 Exercise: Slicing \u00b6 What does the following print: element = 'carbon' print ( 'element[1:3] is:' , element [ 1 : 3 ]) What does thing[low:high] do? What does thing[low:] do? What does thing[:high] do? What does thing[:] do? What about thing[::2] ? thing[::-1] ? Exercise: Fill in the blanks \u00b6 Fill in the blanks so that the program below produces the output shown. values = ____ values . ____ ( 1 ) values . ____ ( 3 ) values . ____ ( 5 ) print ( 'first time:' , values ) values = values [ ____ ] print ( 'second time:' , values ) \u00b6 first time: [1, 3, 5] second time: [3, 5] Exercise: From strings to lists and back \u00b6 Given this: print ( 'string to list:' , list ( 'tin' )) print ( 'list to string:' , '' . join ([ 'g' , 'o' , 'l' , 'd' ])) \u00b6 ['t', 'i', 'n'] 'gold' Explain in simple terms what list('some string') does. What does '-'.join(['x', 'y']) generate? Exercise: Sort and Sorted \u00b6 What do these two programs print? In simple terms, explain the difference between sorted(letters) and letters.sort() . # Program A letters = list ( 'gold' ) result = sorted ( letters ) print ( 'letters is' , letters , 'and result is' , result ) \u00b6 # Program B letters = list ( 'gold' ) result = letters . sort () print ( 'letters is' , letters , 'and result is' , result ) Exercise: Copying (or Not) \u00b6 What do these two programs print? In simple terms, explain the difference between new = old and new = old[:] . # Program A old = list ( 'gold' ) new = old # simple assignment new [ 0 ] = 'D' print ( 'new is' , new , 'and old is' , old ) \u00b6 # Program B old = list ( 'gold' ) new = old [:] # assigning a slice new [ 0 ] = 'D' print ( 'new is' , new , 'and old is' , old )","title":"3   Lists"},{"location":"primer/01_basic_python/3%20-%20Lists/#lesson-3-lists","text":"Teaching: 15min Exercises: 5min","title":"Lesson 3: Lists"},{"location":"primer/01_basic_python/3%20-%20Lists/#a-list-stores-many-values-in-a-single-structure","text":"Doing calculations with a hundred variables called pressure_001 , pressure_002 , etc., would be at least as slow as doing them by hand. Use a list to store many values together. Contained within square brackets [...] . Values separated by commas , . Use len to find out how many values are in a list. pressures = [ 0.273 , 0.275 , 0.277 , 0.275 , 0.276 ] print ( 'pressures:' , pressures ) print ( 'length:' , len ( pressures ))","title":"A list stores many values in a single structure."},{"location":"primer/01_basic_python/3%20-%20Lists/#use-an-items-index-to-fetch-it-from-a-list","text":"just like strings print ( 'zeroth item of pressures:' , pressures [ 0 ]) print ( 'fourth item of pressures:' , pressures [ 4 ]) print ( 'fourth item of \"pressures\":' , \"pressures\" [ 4 ]) Say your name ...","title":"Use an item's index to fetch it from a list"},{"location":"primer/01_basic_python/3%20-%20Lists/#lists-values-can-be-replaced-by-assigning-to-them","text":"Use an index expression on the left of assignment to replace a value. pressures [ 0 ] = 0.265 print ( 'pressures is now:' , pressures ) pressures is now: [0.265, 0.275, 0.277, 0.275, 0.276]","title":"Lists' values can be replaced by assigning to them"},{"location":"primer/01_basic_python/3%20-%20Lists/#appending-items-to-a-list-lengthens-it","text":"use list_name.append to add items to the end of a list. primes = [ 2 , 3 , 5 ] print ( 'primes is initially:' , primes ) primes . append ( 7 ) primes . append ( 9 ) print ( 'primes has become:' , primes ) primes is initially: [2, 3, 5] primes has become: [2, 3, 5, 7, 9] * append is a method of lists * like a function, but tied to a particular object Use object_name.method_name to call methods deliberately resembles the way we refer to things in a library We will meet other methods of lists as we go along Use help(list) for a preview extend is similar to append , but it allows you to combine two lists. teen_primes = [ 11 , 13 , 17 , 19 ] older_primes = [ 23 , 31 , 47 , 59 ] print ( 'primes is currently:' , primes ) primes . extend ( teen_primes ) print ( 'primes has now become:' , primes ) primes . append ( older_primes ) print ( 'primes has finally become:' , primes ) primes is currently: [2, 3, 5, 7, 9] primes has now become: [2, 3, 5, 7, 9, 11, 13, 17, 19] primes has finally become: [2, 3, 5, 7, 9, 11, 13, 17, 19, [23, 31, 47, 59]] Note that while extend maintains the \"flat\" structure of the list, appending a list to a list makes the result two-dimensional.","title":"Appending items to a list lengthens it"},{"location":"primer/01_basic_python/3%20-%20Lists/#use-del-to-remove-items-from-a-list-entirely","text":"del list_name[index] removes an item from a list and shortens the list. Not a function or method, but a statement in the language. print ( 'primes before removing last item:' , primes ) del primes [ 4 ] print ( 'primes after removing last item:' , primes ) primes before removing last item: [2, 3, 5, 7, 9, 11, 13, 17, 19, [23, 31, 47, 59]] primes after removing last item: [2, 3, 5, 7, 11, 13, 17, 19, [23, 31, 47, 59]]","title":"Use del to remove items from a list entirely"},{"location":"primer/01_basic_python/3%20-%20Lists/#the-empty-list-contains-no-values","text":"[] is \"the zero of lists\" Helpful as a starting point for collecting values","title":"The empty list contains no values"},{"location":"primer/01_basic_python/3%20-%20Lists/#lists-may-be-heterogeneous","text":"goals = [ 1 , 'Create lists.' , 2 , 'Extract items from lists.' , 3 , 'Modify lists.' ] * Not always a good idea...","title":"Lists may be heterogeneous"},{"location":"primer/01_basic_python/3%20-%20Lists/#character-strings-are-immutable","text":"Cannot change the characters in a string after it has been created. Python considers the string to be a single value with parts, not a collection of values. import traceback element = 'helium' try : element [ 0 ] = 'C' except TypeError : print ( traceback . format_exc ()) Traceback (most recent call last): File \"<ipython-input-9-7b43d8c316ad>\", line 5, in <module> element[0] = 'C' TypeError: 'str' object does not support item assignment","title":"Character strings are immutable"},{"location":"primer/01_basic_python/3%20-%20Lists/#index-and-slice-to-get-information-out-of-a-stringlist","text":"Locations are numbered from 0 rather than 1 Negative indices count backward from the end of the string Slices include the lower bound but exclude the upper bound, so (upper - lower) is the slice's length print ( 'first character:' , element [ 0 ]) print ( 'last character:' , element [ - 1 ]) print ( 'middle:' , element [ 2 : 5 ]) first character: h last character: m middle: liu Indexing beyond the end of a collection is an error try : print ( '99th element of element is:' , element [ 99 ]) except IndexError : print ( traceback . format_exc ()) Traceback (most recent call last): File \"<ipython-input-11-68966f3afd5b>\", line 2, in <module> print('99th element of element is:', element[99]) IndexError: string index out of range","title":"Index and slice to get information out of a string/list"},{"location":"primer/01_basic_python/3%20-%20Lists/#use-the-built-in-function-len-to-find-the-length-of-a-string","text":"print ( len ( 'helium' )) 6","title":"Use the built-in function len to find the length of a string"},{"location":"primer/01_basic_python/3%20-%20Lists/#exercise-slicing","text":"What does the following print: element = 'carbon' print ( 'element[1:3] is:' , element [ 1 : 3 ]) What does thing[low:high] do? What does thing[low:] do? What does thing[:high] do? What does thing[:] do? What about thing[::2] ? thing[::-1] ?","title":"Exercise: Slicing"},{"location":"primer/01_basic_python/3%20-%20Lists/#exercise-fill-in-the-blanks","text":"Fill in the blanks so that the program below produces the output shown.","title":"Exercise: Fill in the blanks"},{"location":"primer/01_basic_python/3%20-%20Lists/#values-____-values____1-values____3-values____5-printfirst-time-values-values-values____-printsecond-time-values","text":"first time: [1, 3, 5] second time: [3, 5]","title":"values = ____\nvalues.____(1)\nvalues.____(3)\nvalues.____(5)\nprint(&#39;first time:&#39;, values)\nvalues = values[____]\nprint(&#39;second time:&#39;, values)\n"},{"location":"primer/01_basic_python/3%20-%20Lists/#exercise-from-strings-to-lists-and-back","text":"Given this:","title":"Exercise: From strings to lists and back"},{"location":"primer/01_basic_python/3%20-%20Lists/#printstring-to-list-listtin-printlist-to-string-joing-o-l-d","text":"['t', 'i', 'n'] 'gold' Explain in simple terms what list('some string') does. What does '-'.join(['x', 'y']) generate?","title":"print(&#39;string to list:&#39;, list(&#39;tin&#39;))\nprint(&#39;list to string:&#39;, &#39;&#39;.join([&#39;g&#39;, &#39;o&#39;, &#39;l&#39;, &#39;d&#39;]))\n"},{"location":"primer/01_basic_python/3%20-%20Lists/#exercise-sort-and-sorted","text":"What do these two programs print? In simple terms, explain the difference between sorted(letters) and letters.sort() .","title":"Exercise: Sort and Sorted"},{"location":"primer/01_basic_python/3%20-%20Lists/#program-a-letters-listgold-result-sortedletters-printletters-is-letters-and-result-is-result","text":"# Program B letters = list ( 'gold' ) result = letters . sort () print ( 'letters is' , letters , 'and result is' , result )","title":"# Program A\nletters = list(&#39;gold&#39;)\nresult = sorted(letters)\nprint(&#39;letters is&#39;, letters, &#39;and result is&#39;, result)\n"},{"location":"primer/01_basic_python/3%20-%20Lists/#exercise-copying-or-not","text":"What do these two programs print? In simple terms, explain the difference between new = old and new = old[:] .","title":"Exercise: Copying (or Not)"},{"location":"primer/01_basic_python/3%20-%20Lists/#program-a-old-listgold-new-old-simple-assignment-new0-d-printnew-is-new-and-old-is-old","text":"# Program B old = list ( 'gold' ) new = old [:] # assigning a slice new [ 0 ] = 'D' print ( 'new is' , new , 'and old is' , old )","title":"# Program A\nold = list(&#39;gold&#39;)\nnew = old      # simple assignment\nnew[0] = &#39;D&#39;\nprint(&#39;new is&#39;, new, &#39;and old is&#39;, old)\n"},{"location":"primer/01_basic_python/4%20-%20For%20loops/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 4: For loops \u00b6 A for loop can be used to repeat actions over a set of values. name = [ 'J' , 'i' , 'm' , 'm' , 'y' ] for letter in name : print ( letter ) name = 'Jimmy' for letter in name : print ( letter ) J i m m y J i m m y For loops are constructed via the syntax for LOOP_VARIABLE in ITERABLE. The loop variable in the above example I've used is letter and the iterable I've used is the string name , which was defined as 'Jimmy' . Other iterables include lists and dictionary keys, which we'll discuss later. You can construct an iterable corresponding to a set of numbers beginning at 0 and ending at a certain number by using the built-in range function: for number in range ( 6 ): print ( number ) 0 1 2 3 4 5 for number in range ( 2 , 10 ): if number % 2 == 0 : print ( number , \"is even\" ) 2 is even 4 is even 6 is even 8 is even my_sum = 0 for number in range ( 6 ): my_sum += number print ( \"Sum of numbers from 0 to 5 is\" , my_sum ) Sum of numbers from 0 to 5 is 15 for n , letter in enumerate ( name ): print ( \"Letter\" , n , \"-\" , letter ) Letter 0 - J Letter 1 - i Letter 2 - m Letter 3 - m Letter 4 - y Iterables have a length, and you can find this length using the len function. print ( \"Length of\" , name , \"is\" , len ( name )) Length of Jimmy is 5 for letter1 , letter2 in zip ( \"Jimmy\" , \"Heath\" ): print ( letter1 , letter2 ) J H i e m a m t y h Exercise: write a for loop that computes the sum of all of the squares from 2 to 11. \u00b6 s = 0 for i in range ( 10 ): s += pow ( i + 2 , 2 ) print ( s ) 505 Exercise (optional): Write a Python program to construct the following pattern, using a nested for loop. \u00b6 * * * * * * * * * * * * * * * * * * * * * * * * * n = 5 ; for i in range ( n ): for j in range ( i ): print ( '* ' , end = \"\" ) print ( '' ) for i in range ( n , 0 , - 1 ): for j in range ( i ): print ( '* ' , end = \"\" ) print ( '' ) * * * * * * * * * * * * * * * * * * * * * * * * *","title":"4   For loops"},{"location":"primer/01_basic_python/4%20-%20For%20loops/#lesson-4-for-loops","text":"A for loop can be used to repeat actions over a set of values. name = [ 'J' , 'i' , 'm' , 'm' , 'y' ] for letter in name : print ( letter ) name = 'Jimmy' for letter in name : print ( letter ) J i m m y J i m m y For loops are constructed via the syntax for LOOP_VARIABLE in ITERABLE. The loop variable in the above example I've used is letter and the iterable I've used is the string name , which was defined as 'Jimmy' . Other iterables include lists and dictionary keys, which we'll discuss later. You can construct an iterable corresponding to a set of numbers beginning at 0 and ending at a certain number by using the built-in range function: for number in range ( 6 ): print ( number ) 0 1 2 3 4 5 for number in range ( 2 , 10 ): if number % 2 == 0 : print ( number , \"is even\" ) 2 is even 4 is even 6 is even 8 is even my_sum = 0 for number in range ( 6 ): my_sum += number print ( \"Sum of numbers from 0 to 5 is\" , my_sum ) Sum of numbers from 0 to 5 is 15 for n , letter in enumerate ( name ): print ( \"Letter\" , n , \"-\" , letter ) Letter 0 - J Letter 1 - i Letter 2 - m Letter 3 - m Letter 4 - y Iterables have a length, and you can find this length using the len function. print ( \"Length of\" , name , \"is\" , len ( name )) Length of Jimmy is 5 for letter1 , letter2 in zip ( \"Jimmy\" , \"Heath\" ): print ( letter1 , letter2 ) J H i e m a m t y h","title":"Lesson 4: For loops"},{"location":"primer/01_basic_python/4%20-%20For%20loops/#exercise-write-a-for-loop-that-computes-the-sum-of-all-of-the-squares-from-2-to-11","text":"s = 0 for i in range ( 10 ): s += pow ( i + 2 , 2 ) print ( s ) 505","title":"Exercise: write a for loop that computes the sum of all of the squares from 2 to 11."},{"location":"primer/01_basic_python/4%20-%20For%20loops/#exercise-optional-write-a-python-program-to-construct-the-following-pattern-using-a-nested-for-loop","text":"* * * * * * * * * * * * * * * * * * * * * * * * * n = 5 ; for i in range ( n ): for j in range ( i ): print ( '* ' , end = \"\" ) print ( '' ) for i in range ( n , 0 , - 1 ): for j in range ( i ): print ( '* ' , end = \"\" ) print ( '' ) * * * * * * * * * * * * * * * * * * * * * * * * *","title":"Exercise (optional): Write a Python program to construct the following pattern, using a nested for loop."}]}